{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import csv\n",
    "from transformers import BertPreTrainedModel, RobertaConfig\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM, RobertaForSequenceClassification, RobertaModel\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "ROOT_DIR = '~/NLI-KB/'\n",
    "CACHE_DIR = '~/.cache/'\n",
    "CSQA_PATH = 'datasets/csqa.jsonl'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# helper function: read and dump data\n",
    "def dump_jsonl(data, output_path, append=False):\n",
    "    \"\"\"\n",
    "    Write list of objects to a JSON lines file.\n",
    "    \"\"\"\n",
    "    mode = 'a+' if append else 'w'\n",
    "    with open(output_path, mode, encoding='utf-8') as f:\n",
    "        for line in data:\n",
    "            json_record = json.dumps(line, ensure_ascii=False)\n",
    "            f.write(json_record + '\\n')\n",
    "    print('Wrote {} records to {}'.format(len(data), output_path))\n",
    "\n",
    "def load_jsonl(input_path) -> list:\n",
    "    \"\"\"\n",
    "    Read list of objects from a JSON lines file.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line.rstrip('\\n|\\r')))\n",
    "    print('Loaded {} records from {}'.format(len(data), input_path))\n",
    "    return data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from dataclasses import dataclass\n",
    "      \n",
    "class CommonsenseqaSentence(object):\n",
    "    answerKey: int = 1\n",
    "    choicesA: str = None\n",
    "    choicesB: str = None\n",
    "    choicesC: str = None\n",
    "    choicesD: str = None\n",
    "    choicesE: str = None\n",
    "    question: str = None\n",
    "    qid: str = None\n",
    "\n",
    "def load_commonsenseqa_from_path(filepath: str):\n",
    "    csqa = load_jsonl(filepath)\n",
    "    answerToIndex = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4}\n",
    "    csqs_sentences = list()\n",
    "    for c in csqa:\n",
    "        s = CommonsenseqaSentence()\n",
    "        s.qid = c['id']\n",
    "        s.question = c['question']['stem']\n",
    "        s.choicesA = c['question']['choices'][0]['text']\n",
    "        s.choicesB = c['question']['choices'][1]['text']\n",
    "        s.choicesC = c['question']['choices'][2]['text']\n",
    "        s.choicesD = c['question']['choices'][3]['text']\n",
    "        s.choicesE = c['question']['choices'][4]['text']\n",
    "        s.answerKey = answerToIndex[c['answerKey']]\n",
    "        csqs_sentences.append(s)\n",
    "    return csqs_sentences    \n",
    "\n",
    "@dataclass\n",
    "class ExpConfig(object):\n",
    "    # JSONL file path\n",
    "    dataset_path: str = \"\"\n",
    "        \n",
    "    winowhy_dataset_path: str = \"\"\n",
    "        \n",
    "    atomic_dataset_path: str = \"\"\n",
    "        \n",
    "    conceptnet_dataset_path: str = \"\"\n",
    "        \n",
    "    dataset: str = \"winowhy\"\n",
    "    # Task description\n",
    "    task_name: str = \"\"\n",
    "    # Only using single GPU\n",
    "    gpu_id: int = 0\n",
    "    # Seed for random\n",
    "    seed: int = 42\n",
    "    # 'cpu', 'cuda'\n",
    "    device: str = 'cpu' \n",
    "    # \"roberta-base\", \"roberta-largbe\"\n",
    "    model_name: str = \"\"\n",
    "    # If model_path is not None or not empty, load model from model_path instead of transformers' pretrained ones\n",
    "    model_path: str = \"\"\n",
    "    # For training the classifier layer\n",
    "    learning_rate: float = 1e-3\n",
    "    # Number of total epochs\n",
    "    num_training_epochs: int = 15\n",
    "    # Max sequence length\n",
    "    max_seq_len: int = 128\n",
    "        \n",
    "    batch_size: int = 1\n",
    "\n",
    "    def set_seed(self, new_seed = None):\n",
    "        seed = self.seed if new_seed is None else new_seed\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    def set_gpu_if_possible(self, gpu_id = None):\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = 'cuda'\n",
    "            if gpu_id is not None:\n",
    "                self.device = 'cuda:{}'.format(gpu_id)\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "            \n",
    "class RobertaClassificationHead(nn.Module):\n",
    "    \"\"\"Head for sentence-level classification tasks.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "    \n",
    "class RobertaOnlyClassificationHead(BertPreTrainedModel):\n",
    "    config_class = RobertaConfig\n",
    "    base_model_prefix = \"roberta\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.roberta = RobertaModel(config)\n",
    "        self.classifier = RobertaClassificationHead(config)\n",
    "\n",
    "    def forward(self, sequence_output):\n",
    "        logits = self.classifier(sequence_output)\n",
    "        return logits \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Use Roberta (MNLI)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def roberta_cross_entropy_for_csqa_nli(cs_sentence: CommonsenseqaSentence, model: RobertaForSequenceClassification, tokenizer: RobertaTokenizer, max_seq_len: int, device: str='cpu') -> float:\n",
    "\n",
    "    MAX_SEQ_LEN = max_seq_len\n",
    "    question = tokenizer.tokenize(\" Q: \" + cs_sentence.question, add_prefix_space=True)\n",
    "    choicesA = tokenizer.tokenize(\" A: \" + cs_sentence.choicesA, add_prefix_space=True)\n",
    "    choicesB = tokenizer.tokenize(\" A: \" + cs_sentence.choicesB, add_prefix_space=True)\n",
    "    choicesC = tokenizer.tokenize(\" A: \" + cs_sentence.choicesC, add_prefix_space=True)\n",
    "    choicesD = tokenizer.tokenize(\" A: \" + cs_sentence.choicesD, add_prefix_space=True)\n",
    "    choicesE = tokenizer.tokenize(\" A: \" + cs_sentence.choicesE, add_prefix_space=True)\n",
    "\n",
    "    input_ids_A = tokenizer.convert_tokens_to_ids([tokenizer.cls_token] + question + [tokenizer.sep_token] + choicesA + [tokenizer.sep_token])\n",
    "    input_ids_B = tokenizer.convert_tokens_to_ids([tokenizer.cls_token] + question + [tokenizer.sep_token] + choicesB + [tokenizer.sep_token])\n",
    "    input_ids_C = tokenizer.convert_tokens_to_ids([tokenizer.cls_token] + question + [tokenizer.sep_token] + choicesC + [tokenizer.sep_token])\n",
    "    input_ids_D = tokenizer.convert_tokens_to_ids([tokenizer.cls_token] + question + [tokenizer.sep_token] + choicesD + [tokenizer.sep_token])\n",
    "    input_ids_E = tokenizer.convert_tokens_to_ids([tokenizer.cls_token] + question + [tokenizer.sep_token] + choicesE + [tokenizer.sep_token])\n",
    "\n",
    "    input_ids_A = input_ids_A[:MAX_SEQ_LEN]\n",
    "    input_ids_B = input_ids_B[:MAX_SEQ_LEN]\n",
    "    input_ids_C = input_ids_C[:MAX_SEQ_LEN]\n",
    "    input_ids_D = input_ids_D[:MAX_SEQ_LEN]\n",
    "    input_ids_E = input_ids_E[:MAX_SEQ_LEN]\n",
    "    \n",
    "    attention_mask_A = [1] * len(input_ids_A)\n",
    "    attention_mask_B = [1] * len(input_ids_B)\n",
    "    attention_mask_C = [1] * len(input_ids_C)\n",
    "    attention_mask_D = [1] * len(input_ids_D)\n",
    "    attention_mask_E = [1] * len(input_ids_E)\n",
    "    \n",
    "    input_ids_A += [1] * (MAX_SEQ_LEN - len(input_ids_A))\n",
    "    input_ids_B += [1] * (MAX_SEQ_LEN - len(input_ids_B))\n",
    "    input_ids_C += [1] * (MAX_SEQ_LEN - len(input_ids_C))\n",
    "    input_ids_D += [1] * (MAX_SEQ_LEN - len(input_ids_D))\n",
    "    input_ids_E += [1] * (MAX_SEQ_LEN - len(input_ids_E))\n",
    "    \n",
    "    attention_mask_A += [0] * (MAX_SEQ_LEN - len(attention_mask_A))\n",
    "    attention_mask_B += [0] * (MAX_SEQ_LEN - len(attention_mask_B))\n",
    "    attention_mask_C += [0] * (MAX_SEQ_LEN - len(attention_mask_C))\n",
    "    attention_mask_D += [0] * (MAX_SEQ_LEN - len(attention_mask_D))\n",
    "    attention_mask_E += [0] * (MAX_SEQ_LEN - len(attention_mask_E))\n",
    "    \n",
    "    input_ids_A = torch.tensor([input_ids_A]).to(device)\n",
    "    input_ids_B = torch.tensor([input_ids_B]).to(device)\n",
    "    input_ids_C = torch.tensor([input_ids_C]).to(device)\n",
    "    input_ids_D = torch.tensor([input_ids_D]).to(device)\n",
    "    input_ids_E = torch.tensor([input_ids_E]).to(device)\n",
    "    \n",
    "    attention_mask_A = torch.tensor([attention_mask_A]).to(device)\n",
    "    attention_mask_B = torch.tensor([attention_mask_B]).to(device)\n",
    "    attention_mask_C = torch.tensor([attention_mask_C]).to(device)\n",
    "    attention_mask_D = torch.tensor([attention_mask_D]).to(device)\n",
    "    attention_mask_E = torch.tensor([attention_mask_E]).to(device)\n",
    "    \n",
    "    input_ids = torch.tensor([]).to(device).long()\n",
    "    attention_mask = torch.tensor([]).to(device).long()\n",
    "    \n",
    "    input_ids = torch.cat((input_ids, input_ids_A), 0)\n",
    "    input_ids = torch.cat((input_ids, input_ids_B), 0)\n",
    "    input_ids = torch.cat((input_ids, input_ids_C), 0)\n",
    "    input_ids = torch.cat((input_ids, input_ids_D), 0)\n",
    "    input_ids = torch.cat((input_ids, input_ids_E), 0)\n",
    "\n",
    "    attention_mask = torch.cat((attention_mask, attention_mask_A), 0)\n",
    "    attention_mask = torch.cat((attention_mask, attention_mask_B), 0)\n",
    "    attention_mask = torch.cat((attention_mask, attention_mask_C), 0)\n",
    "    attention_mask = torch.cat((attention_mask, attention_mask_D), 0)\n",
    "    attention_mask = torch.cat((attention_mask, attention_mask_E), 0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    return torch.argmax(F.softmax(outputs[0], dim=1).transpose(0, 1), dim=1).to('cpu')[1].item()\n",
    "\n",
    "def roberta_for_csqa(config: ExpConfig, roberta_cross_entropy_for_csqa_nli):\n",
    "\n",
    "    csqa_sentences = load_commonsenseqa_from_path(config.dataset_path)\n",
    "        \n",
    "    if config.model_path is not None and config.model_path != \"\":\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(config.model_path)\n",
    "        model = RobertaForSequenceClassification.from_pretrained(config.model_path, num_labels=3)\n",
    "    else:\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(config.model_name)\n",
    "        model = RobertaForSequenceClassification.from_pretrained(config.model_name)\n",
    "        \n",
    "    model.eval()\n",
    "    model.to(config.device)\n",
    "    \n",
    "    correct = 0\n",
    "    \n",
    "    #result = open(SAVE_DIR + config.model_name + \".csv\", \"w\")\n",
    "    #writer = csv.writer(result)\n",
    "    #writer.writerow([\"pred\", \"label\", \"question\"])\n",
    "    \n",
    "    for cs in csqa_sentences:\n",
    "        pred = roberta_cross_entropy_for_csqa_nli(cs, model, tokenizer, config.max_seq_len, config.device)\n",
    "\n",
    "        if pred == cs.answerKey:\n",
    "            correct += 1\n",
    "        #writer.writerow([pred, cs.answerKey, cs.question])\n",
    "        \n",
    "    acc = correct / len(csqa_sentences)\n",
    "    print(len(csqa_sentences))\n",
    "\n",
    "    return acc"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for model_name in ['roberta-base-mnli', 'roberta-large-mnli']:\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    robertaconfig = ExpConfig()\n",
    "    robertaconfig.set_seed()\n",
    "    robertaconfig.set_gpu_if_possible(0)\n",
    "    robertaconfig.dataset = 'commonsenseqa'\n",
    "    robertaconfig.dataset_path = ROOT_DIR + CSQA_PATH\n",
    "    robertaconfig.task_name = 'Test on CommonsenseQA'\n",
    "    robertaconfig.model_name = model_name\n",
    "    robertaconfig.model_path = CACHE_DIR + model_name\n",
    "    \n",
    "    print('\\n================================')\n",
    "    print('Experiment: {} using {}'.format(robertaconfig.task_name, robertaconfig.model_name))\n",
    "    \n",
    "    scores = roberta_for_csqa(robertaconfig, roberta_cross_entropy_for_csqa_nli)\n",
    "    \n",
    "    print('Scores: {}'.format(scores))\n",
    "    print('================================')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Roberta+MNLI+ATOMIC (full set)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def getInput(question: str, choice: str, atomic: dict, answerIndex:str, tokenizer: RobertaTokenizer, max_seq_len: int, topk: int, device: str='cpu'):\n",
    "    MAX_SEQ_LEN = max_seq_len\n",
    "    \n",
    "    first = tokenizer.tokenize(\" Q: \" + question, add_prefix_space=True)\n",
    "    second = tokenizer.tokenize(\" A: \" + choice, add_prefix_space=True)\n",
    "    \n",
    "    input_ids = torch.tensor([]).to(device).long()\n",
    "    attention_mask = torch.tensor([]).to(device).long()\n",
    "    for i in range(topk):\n",
    "        a = tokenizer.tokenize(\" K: \" + atomic['Overall'][answerIndex][i], add_prefix_space=True)\n",
    "        \n",
    "        input_ids_a = tokenizer.convert_tokens_to_ids([tokenizer.cls_token] + first + [tokenizer.sep_token] + a + [tokenizer.sep_token] + second + [tokenizer.sep_token])\n",
    "        input_ids_a = input_ids_a[:MAX_SEQ_LEN]\n",
    "        input_ids_a += [1] * (MAX_SEQ_LEN - len(input_ids_a))\n",
    "\n",
    "        attention_mask_a = [1] * len(input_ids_a)\n",
    "        attention_mask_a += [0] * (MAX_SEQ_LEN - len(attention_mask_a))\n",
    "\n",
    "        input_ids_a = torch.tensor([input_ids_a]).to(device)\n",
    "        attention_mask_a = torch.tensor([attention_mask_a]).to(device)\n",
    "\n",
    "        input_ids = torch.cat((input_ids, input_ids_a), 0)\n",
    "        attention_mask = torch.cat((attention_mask, attention_mask_a), 0)\n",
    "\n",
    "    return input_ids, attention_mask\n",
    "\n",
    "def roberta_cross_entropy_for_csqa_atomic_nli(question: str, choice: str, atomic: dict, answerIndex: str, model_1: RobertaModel, model_2: RobertaOnlyClassificationHead, tokenizer: RobertaTokenizer, max_seq_len: int, device: str='cpu') -> float:\n",
    "\n",
    "    MAX_SEQ_LEN = 128\n",
    "    input_ids, attention_mask = getInput(question, choice, atomic, answerIndex, tokenizer, MAX_SEQ_LEN, 20, device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model_1.eval()\n",
    "        model_2.eval()\n",
    "        outputs = model_1(input_ids, attention_mask=attention_mask)[0].mean(dim=0, keepdim=True)\n",
    "        logits = model_2(outputs)\n",
    "        loss = F.softmax(logits, dim = 1)\n",
    "        \n",
    "    return loss[0][1].to('cpu').item()\n",
    "\n",
    "def roberta_for_csqa(config: ExpConfig, roberta_cross_entropy_for_csqa_atomic_nli):\n",
    "\n",
    "    csqa_sentences = load_commonsenseqa_from_path(config.dataset_path)\n",
    "    \n",
    "    ff = open(config.atomic_dataset_path) \n",
    "    atomic_sentences = json.loads(ff.read())\n",
    "    ff.close()\n",
    "        \n",
    "    if config.model_path is not None and config.model_path != \"\":\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(config.model_path)\n",
    "        model_1 = RobertaModel.from_pretrained(config.model_path)\n",
    "        model_2 = RobertaOnlyClassificationHead.from_pretrained(config.model_path, num_labels=3)\n",
    "    else:\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "        model_1 = RobertaModel.from_pretrained(config.model_path)\n",
    "        model_2 = RobertaOnlyClassificationHead.from_pretrained(config.model_path)\n",
    "        \n",
    "    model_1.eval()\n",
    "    model_1.to(config.device)\n",
    "    \n",
    "    model_2.eval()\n",
    "    model_2.to(config.device)\n",
    "    \n",
    "    correct = 0\n",
    "    \n",
    "    #result = open(SAVE_DIR + config.model_name + \".csv\", \"w\")\n",
    "    #writer = csv.writer(result)\n",
    "    #writer.writerow([\"pred\", \"label\", \"question\"])\n",
    "    \n",
    "    for i in range(len(csqa_sentences)):\n",
    "        lossA = roberta_cross_entropy_for_csqa_atomic_nli(csqa_sentences[i].question, csqa_sentences[i].choicesA, atomic_sentences[str(i)], 'A', model_1, model_2, tokenizer, config.max_seq_len, config.device)\n",
    "        lossB = roberta_cross_entropy_for_csqa_atomic_nli(csqa_sentences[i].question, csqa_sentences[i].choicesB, atomic_sentences[str(i)], 'B', model_1, model_2, tokenizer, config.max_seq_len, config.device)\n",
    "        lossC = roberta_cross_entropy_for_csqa_atomic_nli(csqa_sentences[i].question, csqa_sentences[i].choicesC, atomic_sentences[str(i)], 'C', model_1, model_2, tokenizer, config.max_seq_len, config.device)\n",
    "        lossD = roberta_cross_entropy_for_csqa_atomic_nli(csqa_sentences[i].question, csqa_sentences[i].choicesD, atomic_sentences[str(i)], 'D', model_1, model_2, tokenizer, config.max_seq_len, config.device)\n",
    "        lossE = roberta_cross_entropy_for_csqa_atomic_nli(csqa_sentences[i].question, csqa_sentences[i].choicesE, atomic_sentences[str(i)], 'E', model_1, model_2, tokenizer, config.max_seq_len, config.device)\n",
    "        scores = [lossA, lossB, lossC, lossD, lossE]\n",
    "        pred = scores.index(max(scores))\n",
    "        if pred == csqa_sentences[i].answerKey:\n",
    "            correct += 1\n",
    "        #writer.writerow([pred, csqa_sentences[i].answerKey, csqa_sentences[i].question])\n",
    "    acc = correct / len(csqa_sentences)\n",
    "\n",
    "    return acc"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "KB_DIR = \"kb_extract/csqa_atomic/\"\n",
    "for model_name in ['roberta-base-mnli']:\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    robertaconfig = ExpConfig()\n",
    "    robertaconfig.set_seed()\n",
    "    robertaconfig.set_gpu_if_possible(0)\n",
    "    robertaconfig.dataset = 'commonsenseqa'\n",
    "    robertaconfig.dataset_path = ROOT_DIR + CSQA_PATH\n",
    "    robertaconfig.atomic_dataset_path = ROOT_DIR + KB_DIR + model_name + \"-atomic.json\"\n",
    "    robertaconfig.task_name = 'Test on CommonsenseQA'\n",
    "    robertaconfig.model_name = model_name\n",
    "    robertaconfig.model_path = CACHE_DIR + model_name\n",
    "    \n",
    "    print('\\n================================')\n",
    "    print('Experiment: {} using {}'.format(robertaconfig.task_name, robertaconfig.model_name))\n",
    "    \n",
    "    scores = roberta_for_csqa(robertaconfig, roberta_cross_entropy_for_csqa_atomic_nli)\n",
    "    \n",
    "    print('Scores: {}'.format(scores))\n",
    "    print('================================')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Roberta+MNLI+ConceptNet (full set)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def getInput(question: str, choice: str, conceptnet: dict, answerIndex:str, tokenizer: RobertaTokenizer, max_seq_len: int, topk: int, device: str='cpu'):\n",
    "    MAX_SEQ_LEN = max_seq_len\n",
    "    \n",
    "    first = tokenizer.tokenize(\" Q: \" + question, add_prefix_space=True)\n",
    "    second = tokenizer.tokenize(\" A: \" + choice, add_prefix_space=True)\n",
    "    \n",
    "    input_ids = torch.tensor([]).to(device).long()\n",
    "    attention_mask = torch.tensor([]).to(device).long()\n",
    "    for i in range(topk):\n",
    "        a = tokenizer.tokenize(\" K: \" + conceptnet['Overall'][answerIndex][i], add_prefix_space=True)\n",
    "        \n",
    "        input_ids_a = tokenizer.convert_tokens_to_ids([tokenizer.cls_token] + first + [tokenizer.sep_token] + a + [tokenizer.sep_token] + second + [tokenizer.sep_token])\n",
    "        input_ids_a = input_ids_a[:MAX_SEQ_LEN]\n",
    "        input_ids_a += [1] * (MAX_SEQ_LEN - len(input_ids_a))\n",
    "\n",
    "        attention_mask_a = [1] * len(input_ids_a)\n",
    "        attention_mask_a += [0] * (MAX_SEQ_LEN - len(attention_mask_a))\n",
    "\n",
    "        input_ids_a = torch.tensor([input_ids_a]).to(device)\n",
    "        attention_mask_a = torch.tensor([attention_mask_a]).to(device)\n",
    "\n",
    "        input_ids = torch.cat((input_ids, input_ids_a), 0)\n",
    "        attention_mask = torch.cat((attention_mask, attention_mask_a), 0)\n",
    "\n",
    "    return input_ids, attention_mask\n",
    "\n",
    "def roberta_cross_entropy_for_csqa_conceptnet_nli(question: str, choice: str, conceptnet: dict, answerIndex: str, model_1: RobertaModel, model_2: RobertaOnlyClassificationHead, tokenizer: RobertaTokenizer, max_seq_len: int, device: str='cpu') -> float:\n",
    "\n",
    "    MAX_SEQ_LEN = 128\n",
    "    input_ids, attention_mask = getInput(question, choice, conceptnet, answerIndex, tokenizer, MAX_SEQ_LEN, 20, device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model_1.eval()\n",
    "        model_2.eval()\n",
    "        outputs = model_1(input_ids, attention_mask=attention_mask)[0].mean(dim=0, keepdim=True)\n",
    "        logits = model_2(outputs)\n",
    "        loss = F.softmax(logits, dim = 1)\n",
    "        \n",
    "    return loss[0][1].to('cpu').item()\n",
    "\n",
    "def roberta_for_csqa(config: ExpConfig, roberta_cross_entropy_for_csqa_conceptnet_nli):\n",
    "\n",
    "    csqa_sentences = load_commonsenseqa_from_path(config.dataset_path)\n",
    "    \n",
    "    ff = open(config.conceptnet_dataset_path) \n",
    "    conceptnet_sentences = json.loads(ff.read())\n",
    "    ff.close()\n",
    "        \n",
    "    if config.model_path is not None and config.model_path != \"\":\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(config.model_path)\n",
    "        model_1 = RobertaModel.from_pretrained(config.model_path)\n",
    "        model_2 = RobertaOnlyClassificationHead.from_pretrained(config.model_path, num_labels=3)\n",
    "    else:\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "        model_1 = RobertaModel.from_pretrained(config.model_path)\n",
    "        model_2 = RobertaOnlyClassificationHead.from_pretrained(config.model_path)\n",
    "        \n",
    "    model_1.eval()\n",
    "    model_1.to(config.device)\n",
    "    \n",
    "    model_2.eval()\n",
    "    model_2.to(config.device)\n",
    "    \n",
    "    correct = 0\n",
    "    \n",
    "    #result = open(SAVE_DIR + config.model_name + \".csv\", \"w\")\n",
    "    #writer = csv.writer(result)\n",
    "    #writer.writerow([\"pred\", \"label\", \"question\"])\n",
    "    \n",
    "    for i in range(len(csqa_sentences)):\n",
    "        lossA = roberta_cross_entropy_for_csqa_conceptnet_nli(csqa_sentences[i].question, csqa_sentences[i].choicesA, conceptnet_sentences[str(i)], 'A', model_1, model_2, tokenizer, config.max_seq_len, config.device)\n",
    "        lossB = roberta_cross_entropy_for_csqa_conceptnet_nli(csqa_sentences[i].question, csqa_sentences[i].choicesB, conceptnet_sentences[str(i)], 'B', model_1, model_2, tokenizer, config.max_seq_len, config.device)\n",
    "        lossC = roberta_cross_entropy_for_csqa_conceptnet_nli(csqa_sentences[i].question, csqa_sentences[i].choicesC, conceptnet_sentences[str(i)], 'C', model_1, model_2, tokenizer, config.max_seq_len, config.device)\n",
    "        lossD = roberta_cross_entropy_for_csqa_conceptnet_nli(csqa_sentences[i].question, csqa_sentences[i].choicesD, conceptnet_sentences[str(i)], 'D', model_1, model_2, tokenizer, config.max_seq_len, config.device)\n",
    "        lossE = roberta_cross_entropy_for_csqa_conceptnet_nli(csqa_sentences[i].question, csqa_sentences[i].choicesE, conceptnet_sentences[str(i)], 'E', model_1, model_2, tokenizer, config.max_seq_len, config.device)\n",
    "        scores = [lossA, lossB, lossC, lossD, lossE]\n",
    "        pred = scores.index(max(scores))\n",
    "        if pred == csqa_sentences[i].answerKey:\n",
    "            correct += 1\n",
    "        #writer.writerow([pred, csqa_sentences[i].answerKey, csqa_sentences[i].question])\n",
    "    acc = correct / len(csqa_sentences)\n",
    "\n",
    "    return acc"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "KB_DIR = \"kb_extract/csqa_cn/\"\n",
    "for model_name in ['roberta-base-mnli']:\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    robertaconfig = ExpConfig()\n",
    "    robertaconfig.set_seed()\n",
    "    robertaconfig.set_gpu_if_possible(0)\n",
    "    robertaconfig.dataset = 'commonsenseqa'\n",
    "    robertaconfig.dataset_path = ROOT_DIR + CSQA_PATH\n",
    "    robertaconfig.conceptnet_dataset_path = ROOT_DIR + KB_DIR + model_name + \"-conceptnet.json\"\n",
    "    robertaconfig.task_name = 'Test on CommonsenseQA'\n",
    "    robertaconfig.model_name = model_name\n",
    "    robertaconfig.model_path = CACHE_DIR + model_name\n",
    "    \n",
    "    print('\\n================================')\n",
    "    print('Experiment: {} using {}'.format(robertaconfig.task_name, robertaconfig.model_name))\n",
    "    \n",
    "    scores = roberta_for_csqa(robertaconfig, roberta_cross_entropy_for_csqa_conceptnet_nli)\n",
    "    \n",
    "    print('Scores: {}'.format(scores))\n",
    "    print('================================')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wino",
   "language": "python",
   "name": "wino"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}