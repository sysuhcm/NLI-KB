{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import csv\n",
    "from transformers import BertPreTrainedModel, RobertaConfig\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM, RobertaForSequenceClassification, RobertaModel\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "ROOT_DIR = '~/NLI+KB/'\n",
    "CACHE_DIR = '~/.cache/'\n",
    "WINOWHY_PATH = 'datasets/winowhy/winowhy.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function: read and dump data\n",
    "def dump_jsonl(data, output_path, append=False):\n",
    "    \"\"\"\n",
    "    Write list of objects to a JSON lines file.\n",
    "    \"\"\"\n",
    "    mode = 'a+' if append else 'w'\n",
    "    with open(output_path, mode, encoding='utf-8') as f:\n",
    "        for line in data:\n",
    "            json_record = json.dumps(line, ensure_ascii=False)\n",
    "            f.write(json_record + '\\n')\n",
    "    print('Wrote {} records to {}'.format(len(data), output_path))\n",
    "\n",
    "def load_jsonl(input_path) -> list:\n",
    "    \"\"\"\n",
    "    Read list of objects from a JSON lines file.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line.rstrip('\\n|\\r')))\n",
    "    print('Loaded {} records from {}'.format(len(data), input_path))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "class WinoWhySentence(object):\n",
    "    sentence: str = None\n",
    "    context: str = None\n",
    "    wsc_sentence: str = None\n",
    "    answer_reason: str = None\n",
    "    reason: str = None\n",
    "    label: int = 0\n",
    "    wsc_id: int = 0\n",
    "    fold_num: int = 1\n",
    "    wsc_marked_sentence: str = None\n",
    "    wsc_asked_sentence: str = None\n",
    "        \n",
    "def load_winowhy_from_path(filepath: str):\n",
    "    ws = load_jsonl(filepath)\n",
    "    winowhy_sentences = list()\n",
    "    for w in ws:\n",
    "        s = WinoWhySentence()\n",
    "        s.sentence = w['sentence']\n",
    "        s.context = w['context']\n",
    "        s.wsc_sentence = w['wsc_sentence']\n",
    "        s.answer_reason = w['answer_reason']\n",
    "        s.reason = w['reason']\n",
    "        s.label = w['label']\n",
    "        s.wsc_id = w['wsc_id']\n",
    "        s.fold_num = w['fold_num']\n",
    "        s.wsc_marked_sentence = w['wsc_marked_sentence']\n",
    "        s.wsc_asked_sentence = w['wsc_asked_sentence']\n",
    "        winowhy_sentences.append(s)\n",
    "    return winowhy_sentences\n",
    "\n",
    "@dataclass\n",
    "class ExpConfig(object):\n",
    "    # JSONL file path\n",
    "    dataset_path: str = \"\"\n",
    "        \n",
    "    winowhy_dataset_path: str = \"\"\n",
    "        \n",
    "    atomic_dataset_path: str = \"\"\n",
    "        \n",
    "    conceptnet_dataset_path: str = \"\"\n",
    "\n",
    "    dataset: str = \"winowhy\"\n",
    "    # Task description\n",
    "    task_name: str = \"\"\n",
    "    # Only using single GPU\n",
    "    gpu_id: int = 0\n",
    "    # Seed for random\n",
    "    seed: int = 42\n",
    "    # 'cpu', 'cuda'\n",
    "    device: str = 'cpu' \n",
    "    # \"roberta-base\", \"roberta-largbe\"\n",
    "    model_name: str = \"\"\n",
    "    # If model_path is not None or not empty, load model from model_path instead of transformers' pretrained ones\n",
    "    model_path: str = \"\"\n",
    "    # For training the classifier layer\n",
    "    learning_rate: float = 1e-3\n",
    "    # Number of total epochs\n",
    "    num_training_epochs: int = 15\n",
    "    # Max sequence length\n",
    "    max_seq_len: int = 128\n",
    "        \n",
    "    batch_size: int = 1\n",
    "\n",
    "    def set_seed(self, new_seed = None):\n",
    "        seed = self.seed if new_seed is None else new_seed\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    def set_gpu_if_possible(self, gpu_id = None):\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = 'cuda'\n",
    "            if gpu_id is not None:\n",
    "                self.device = 'cuda:{}'.format(gpu_id)\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "            \n",
    "class RobertaClassificationHead(nn.Module):\n",
    "    \"\"\"Head for sentence-level classification tasks.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "    \n",
    "class RobertaOnlyClassificationHead(BertPreTrainedModel):\n",
    "    config_class = RobertaConfig\n",
    "    base_model_prefix = \"roberta\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.roberta = RobertaModel(config)\n",
    "        self.classifier = RobertaClassificationHead(config)\n",
    "\n",
    "    def forward(self, sequence_output):\n",
    "        logits = self.classifier(sequence_output)\n",
    "        return logits \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RoBERTa + QNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roberta_cross_entropy_for_winowhy_nli(wl_sentence: WinoWhySentence, model: RobertaForSequenceClassification, tokenizer: RobertaTokenizer, max_seq_len: int, device: str='cpu') -> float:\n",
    "\n",
    "    MAX_SEQ_LEN = max_seq_len \n",
    "    \n",
    "    first = tokenizer.tokenize(wl_sentence.reason, add_prefix_space=True)\n",
    "    second = tokenizer.tokenize(wl_sentence.wsc_marked_sentence, add_prefix_space=True)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids([tokenizer.cls_token] + first + [tokenizer.sep_token] + second + [tokenizer.sep_token])\n",
    "    input_ids = input_ids[:MAX_SEQ_LEN]\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "    \n",
    "    input_ids += [1] * (MAX_SEQ_LEN - len(input_ids))\n",
    "    attention_mask += [0] * (MAX_SEQ_LEN - len(attention_mask))\n",
    "    \n",
    "    input_ids = torch.tensor([input_ids]).to(device)\n",
    "    attention_mask = torch.tensor([attention_mask]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "    return torch.argmax(outputs[0]).to('cpu').item()\n",
    "\n",
    "def roberta_for_winowhy(config: ExpConfig, roberta_cross_entropy_for_winowhy_nli):\n",
    "    \n",
    "    winowhy_sentences = load_winowhy_from_path(config.dataset_path)\n",
    "        \n",
    "    # Init the model, tokenizer\n",
    "    if config.model_path is not None and config.model_path != \"\":\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(config.model_path)\n",
    "        model = RobertaForSequenceClassification.from_pretrained(config.model_path)\n",
    "    else:\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(config.model_name)\n",
    "        model = RobertaForSequenceClassification.from_pretrained(config.model_name)\n",
    "        \n",
    "    model.eval()\n",
    "    model.to(config.device)\n",
    "    \n",
    "    correct = 0\n",
    "    \n",
    "    #result = open(SAVE_DIR + config.model_name + \".csv\", \"w\")\n",
    "    #writer = csv.writer(result)\n",
    "    #writer.writerow([\"pred\", \"label\", \"ww_sentence\", \"wsc_id\"])\n",
    "    \n",
    "    for ws in winowhy_sentences:\n",
    "        pred = roberta_cross_entropy_for_winowhy_nli(ws, model, tokenizer, config.max_seq_len, config.device)\n",
    "        if pred == 0:\n",
    "            pred_str = \"entailment\"\n",
    "        elif pred == 1:\n",
    "            pred_str = \"not entailment\"\n",
    "        if pred != ws.label:\n",
    "            correct += 1\n",
    "        #writer.writerow([pred_str, ws.label, ws.sentence, ws.wsc_id])\n",
    "        \n",
    "    acc = correct / len(winowhy_sentences)\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in ['roberta-base-qnli', 'roberta-large-qnli']:\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    robertaconfig = ExpConfig()\n",
    "    robertaconfig.set_seed()\n",
    "    robertaconfig.set_gpu_if_possible(0)\n",
    "    robertaconfig.dataset = 'winowhy'\n",
    "    robertaconfig.dataset_path = ROOT_DIR + WINOWHY_PATH\n",
    "    robertaconfig.task_name = 'Test on WinoWhy'\n",
    "    robertaconfig.model_name = model_name\n",
    "    robertaconfig.model_path = CACHE_DIR + model_name\n",
    "    \n",
    "    print('\\n================================')\n",
    "    print('Experiment: {} using {}'.format(robertaconfig.task_name, robertaconfig.model_name))\n",
    "    \n",
    "    scores = roberta_for_winowhy(robertaconfig, roberta_cross_entropy_for_winowhy_nli)\n",
    "    \n",
    "    print('Scores: {}'.format(scores))\n",
    "    print('================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roberta+QNLI+ATOMIC(full set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInput(wl_sentence: WinoWhySentence, atomic: dict, tokenizer: RobertaTokenizer, max_seq_len: int, topk: int, device: str='cpu'):\n",
    "    MAX_SEQ_LEN = max_seq_len\n",
    "    \n",
    "    first = tokenizer.tokenize(wl_sentence.reason, add_prefix_space=True)\n",
    "    second = tokenizer.tokenize(wl_sentence.wsc_marked_sentence, add_prefix_space=True)\n",
    "    \n",
    "    input_ids = torch.tensor([]).to(device).long()\n",
    "    attention_mask = torch.tensor([]).to(device).long()\n",
    "    \n",
    "    for i in range(topk):\n",
    "        a = tokenizer.tokenize(atomic['Overall'][i], add_prefix_space=True)\n",
    "        input_ids_a = tokenizer.convert_tokens_to_ids([tokenizer.cls_token] + first + [tokenizer.sep_token] + a + [tokenizer.sep_token] + second + [tokenizer.sep_token])\n",
    "\n",
    "        input_ids_a = input_ids_a[:MAX_SEQ_LEN]\n",
    "        attention_mask_a = [1] * len(input_ids_a)\n",
    "        \n",
    "        input_ids_a += [1] * (MAX_SEQ_LEN - len(input_ids_a))\n",
    "        attention_mask_a += [0] * (MAX_SEQ_LEN - len(attention_mask_a))\n",
    "\n",
    "        input_ids_a = torch.tensor([input_ids_a]).to(device)\n",
    "        attention_mask_a = torch.tensor([attention_mask_a]).to(device)\n",
    "        \n",
    "        input_ids = torch.cat((input_ids, input_ids_a), 0)\n",
    "        attention_mask = torch.cat((attention_mask, attention_mask_a), 0)\n",
    "\n",
    "    return input_ids, attention_mask\n",
    "\n",
    "def roberta_cross_entropy_for_winowhy_atomic_nli(wl_sentence: WinoWhySentence, atomic: dict, model_1: RobertaModel, model_2: RobertaOnlyClassificationHead, tokenizer: RobertaTokenizer, max_seq_len: int, device: str='cpu') -> float:\n",
    "\n",
    "    MAX_SEQ_LEN = 128\n",
    "    input_ids, attention_mask = getInput(wl_sentence, atomic, tokenizer, MAX_SEQ_LEN, 10, device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model_1.eval()\n",
    "        model_2.eval()\n",
    "        outputs = model_1(input_ids, attention_mask=attention_mask)[0].mean(dim=0, keepdim=True)\n",
    "        logits = model_2(outputs)\n",
    "        loss = F.softmax(logits, dim = 1)\n",
    "    return torch.argmax(logits).to('cpu').item(), loss[0][0].item(), loss[0][1].item()\n",
    "\n",
    "def roberta_for_winowhy(config: ExpConfig, roberta_cross_entropy_for_winowhy_atomic_nli):\n",
    "    \n",
    "    winowhy_sentences = load_winowhy_from_path(config.winowhy_dataset_path)\n",
    "    \n",
    "    ff = open(config.atomic_dataset_path) \n",
    "    atomic_sentences = json.loads(ff.read())\n",
    "    ff.close()\n",
    "        \n",
    "    if config.model_path is not None and config.model_path != \"\":\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(config.model_path)\n",
    "        model_1 = RobertaModel.from_pretrained(config.model_path)\n",
    "        model_2 = RobertaOnlyClassificationHead.from_pretrained(config.model_path)\n",
    "    else:\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "        model_1 = RobertaModel.from_pretrained(config.model_path)\n",
    "        model_2 = RobertaOnlyClassificationHead.from_pretrained(config.model_path)\n",
    "        \n",
    "    model_1.eval()\n",
    "    model_1.to(config.device)\n",
    "    \n",
    "    model_2.eval()\n",
    "    model_2.to(config.device)\n",
    "    \n",
    "    correct = 0\n",
    "    \n",
    "    #result = open(SAVE_DIR + config.model_name + \".csv\", \"w\")\n",
    "    #writer = csv.writer(result)\n",
    "    #writer.writerow([\"pred\", \"label\", \"ww_sentence\", \"wsc_id\"])\n",
    "    \n",
    "    for i in range(len(winowhy_sentences)):\n",
    "        pred, entail_logits, notentail_logits = roberta_cross_entropy_for_winowhy_atomic_nli(winowhy_sentences[i], atomic_sentences[str(i)], model_1, model_2, tokenizer, config.max_seq_len, config.device)\n",
    "        if pred == 0:\n",
    "            pred_str = \"entailment\"\n",
    "        elif pred == 1:\n",
    "            pred_str = \"not entailment\"\n",
    "        if pred != winowhy_sentences[i].label:\n",
    "            correct += 1\n",
    "        #writer.writerow([pred_str, winowhy_sentences[i].label, winowhy_sentences[i].sentence, winowhy_sentences[i].wsc_id])\n",
    "        \n",
    "    acc = correct / len(winowhy_sentences)\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KB_DIR = \"kb_extract/winowhy_atomic/\"\n",
    "for model_name in ['roberta-base-qnli']:\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    robertaconfig = ExpConfig()\n",
    "    robertaconfig.set_seed()\n",
    "    robertaconfig.set_gpu_if_possible(0)\n",
    "    robertaconfig.dataset = 'winowhy'\n",
    "    robertaconfig.winowhy_dataset_path = ROOT_DIR + WINOWHY_PATH\n",
    "    robertaconfig.atomic_dataset_path = ROOT_DIR + KB_DIR + model_name + \"-atomic.json\"\n",
    "    robertaconfig.task_name = 'Test on WinoWhy'\n",
    "    robertaconfig.model_name = model_name\n",
    "    robertaconfig.model_path = CACHE_DIR + model_name\n",
    "    \n",
    "    print('\\n================================')\n",
    "    print('Experiment: {} using {}'.format(robertaconfig.task_name, robertaconfig.model_name))\n",
    "    \n",
    "    scores = roberta_for_winowhy(robertaconfig, roberta_cross_entropy_for_winowhy_atomic_nli)\n",
    "    \n",
    "    print('Scores: {}'.format(scores))\n",
    "    print('================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RoBERTa+QNLI+ATOMIC (five categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInput(wl_sentence: WinoWhySentence, atomic: dict, tokenizer: RobertaTokenizer, max_seq_len: int, topk: int, device: str='cpu'):\n",
    "    MAX_SEQ_LEN = max_seq_len\n",
    "\n",
    "    first = tokenizer.tokenize(wl_sentence.reason, add_prefix_space=True)\n",
    "    second = tokenizer.tokenize(wl_sentence.wsc_marked_sentence, add_prefix_space=True)\n",
    "    \n",
    "    input_ids = torch.tensor([]).to(device).long()\n",
    "    attention_mask = torch.tensor([]).to(device).long()\n",
    "    for i in range(topk):\n",
    "        a = tokenizer.tokenize(atomic['Physical-Entity'][i], add_prefix_space=True)\n",
    "        b = tokenizer.tokenize(atomic['Event-Centered'][i], add_prefix_space=True)\n",
    "        c = tokenizer.tokenize(atomic['MentalState'][i], add_prefix_space=True)\n",
    "        d = tokenizer.tokenize(atomic['Persona'][i], add_prefix_space=True)\n",
    "        e = tokenizer.tokenize(atomic['Behavior'][i], add_prefix_space=True)\n",
    "        \n",
    "        input_ids_a = tokenizer.convert_tokens_to_ids([tokenizer.cls_token] + first + [tokenizer.sep_token] + a + [tokenizer.sep_token] + second + [tokenizer.sep_token])\n",
    "        input_ids_b = tokenizer.convert_tokens_to_ids([tokenizer.cls_token] + first + [tokenizer.sep_token] + b + [tokenizer.sep_token] + second + [tokenizer.sep_token])\n",
    "        input_ids_c = tokenizer.convert_tokens_to_ids([tokenizer.cls_token] + first + [tokenizer.sep_token] + c + [tokenizer.sep_token] + second + [tokenizer.sep_token])\n",
    "        input_ids_d = tokenizer.convert_tokens_to_ids([tokenizer.cls_token] + first + [tokenizer.sep_token] + d + [tokenizer.sep_token] + second + [tokenizer.sep_token])\n",
    "        input_ids_e = tokenizer.convert_tokens_to_ids([tokenizer.cls_token] + first + [tokenizer.sep_token] + e + [tokenizer.sep_token] + second + [tokenizer.sep_token])\n",
    "\n",
    "        input_ids_a = input_ids_a[:MAX_SEQ_LEN]\n",
    "        input_ids_b = input_ids_b[:MAX_SEQ_LEN]\n",
    "        input_ids_c = input_ids_c[:MAX_SEQ_LEN]\n",
    "        input_ids_d = input_ids_d[:MAX_SEQ_LEN]\n",
    "        input_ids_e = input_ids_e[:MAX_SEQ_LEN]\n",
    "\n",
    "        attention_mask_a = [1] * len(input_ids_a)\n",
    "        attention_mask_b = [1] * len(input_ids_b)\n",
    "        attention_mask_c = [1] * len(input_ids_c)\n",
    "        attention_mask_d = [1] * len(input_ids_d)\n",
    "        attention_mask_e = [1] * len(input_ids_e)\n",
    "\n",
    "        input_ids_a += [1] * (MAX_SEQ_LEN - len(input_ids_a))\n",
    "        input_ids_b += [1] * (MAX_SEQ_LEN - len(input_ids_b))\n",
    "        input_ids_c += [1] * (MAX_SEQ_LEN - len(input_ids_c))\n",
    "        input_ids_d += [1] * (MAX_SEQ_LEN - len(input_ids_d))\n",
    "        input_ids_e += [1] * (MAX_SEQ_LEN - len(input_ids_e))\n",
    "\n",
    "        attention_mask_a += [0] * (MAX_SEQ_LEN - len(attention_mask_a))\n",
    "        attention_mask_b += [0] * (MAX_SEQ_LEN - len(attention_mask_b))\n",
    "        attention_mask_c += [0] * (MAX_SEQ_LEN - len(attention_mask_c))\n",
    "        attention_mask_d += [0] * (MAX_SEQ_LEN - len(attention_mask_d))\n",
    "        attention_mask_e += [0] * (MAX_SEQ_LEN - len(attention_mask_e))\n",
    "\n",
    "        input_ids_a = torch.tensor([input_ids_a]).to(device)\n",
    "        input_ids_b = torch.tensor([input_ids_b]).to(device)\n",
    "        input_ids_c = torch.tensor([input_ids_c]).to(device)\n",
    "        input_ids_d = torch.tensor([input_ids_d]).to(device)\n",
    "        input_ids_e = torch.tensor([input_ids_e]).to(device)\n",
    "\n",
    "        attention_mask_a = torch.tensor([attention_mask_a]).to(device)\n",
    "        attention_mask_b = torch.tensor([attention_mask_b]).to(device)\n",
    "        attention_mask_c = torch.tensor([attention_mask_c]).to(device)\n",
    "        attention_mask_d = torch.tensor([attention_mask_d]).to(device)\n",
    "        attention_mask_e = torch.tensor([attention_mask_e]).to(device)\n",
    "        \n",
    "        input_ids = torch.cat((input_ids, input_ids_a), 0)\n",
    "        #input_ids = torch.cat((input_ids, input_ids_b), 0)\n",
    "        #input_ids = torch.cat((input_ids, input_ids_c), 0)\n",
    "        #input_ids = torch.cat((input_ids, input_ids_d), 0)\n",
    "        #input_ids = torch.cat((input_ids, input_ids_e), 0)\n",
    "        \n",
    "        attention_mask = torch.cat((attention_mask, attention_mask_a), 0)\n",
    "        #attention_mask = torch.cat((attention_mask, attention_mask_b), 0)\n",
    "        #attention_mask = torch.cat((attention_mask, attention_mask_c), 0)\n",
    "        #attention_mask = torch.cat((attention_mask, attention_mask_d), 0)\n",
    "        #attention_mask = torch.cat((attention_mask, attention_mask_e), 0)\n",
    "\n",
    "    return input_ids, attention_mask\n",
    "\n",
    "def roberta_cross_entropy_for_winowhy_atomic_nli(wl_sentence: WinoWhySentence, atomic: dict, model_1: RobertaModel, model_2: RobertaOnlyClassificationHead, tokenizer: RobertaTokenizer, max_seq_len: int, device: str='cpu') -> float:\n",
    "\n",
    "    MAX_SEQ_LEN = 128\n",
    "    input_ids, attention_mask = getInput(wl_sentence, atomic, tokenizer, MAX_SEQ_LEN, 10, device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model_1.eval()\n",
    "        model_2.eval()\n",
    "        outputs = model_1(input_ids, attention_mask=attention_mask)[0].mean(dim=0, keepdim=True)\n",
    "        logits = model_2(outputs)\n",
    "        loss = F.softmax(logits, dim = 1)\n",
    "    return torch.argmax(logits).to('cpu').item(), loss[0][0].item(), loss[0][1].item()\n",
    "\n",
    "def roberta_for_winowhy(config: ExpConfig, roberta_cross_entropy_for_winowhy_atomic_nli):\n",
    "\n",
    "    winowhy_sentences = load_winowhy_from_path(config.winowhy_dataset_path)\n",
    "    \n",
    "    \n",
    "    ff = open(config.atomic_dataset_path) \n",
    "    atomic_sentences = json.loads(ff.read())\n",
    "    ff.close()\n",
    "        \n",
    "    if config.model_path is not None and config.model_path != \"\":\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(config.model_path)\n",
    "        model_1 = RobertaModel.from_pretrained(config.model_path)\n",
    "        model_2 = RobertaOnlyClassificationHead.from_pretrained(config.model_path)\n",
    "    else:\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "        model_1 = RobertaModel.from_pretrained(config.model_path)\n",
    "        model_2 = RobertaOnlyClassificationHead.from_pretrained(config.model_path)\n",
    "        \n",
    "    model_1.eval()\n",
    "    model_1.to(config.device)\n",
    "    \n",
    "    model_2.eval()\n",
    "    model_2.to(config.device)\n",
    "    \n",
    "    correct = 0\n",
    "    \n",
    "    #result = open(SAVE_DIR + config.model_name + \".csv\", \"w\")\n",
    "    #writer = csv.writer(result)\n",
    "    #writer.writerow([\"pred\", \"label\", \"ww_sentence\", \"wsc_id\"])\n",
    "    \n",
    "    for i in range(len(winowhy_sentences)):\n",
    "        \n",
    "        pred, entail_logits, notentail_logits = roberta_cross_entropy_for_winowhy_atomic_nli(winowhy_sentences[i], atomic_sentences[str(i)], model_1, model_2, tokenizer, config.max_seq_len, config.device)\n",
    "        if pred == 0:\n",
    "            pred_str = \"entailment\"\n",
    "        elif pred == 1:\n",
    "            pred_str = \"not entailment\"\n",
    "        if pred != winowhy_sentences[i].label:\n",
    "            correct += 1\n",
    "        #writer.writerow([pred_str, winowhy_sentences[i].label, winowhy_sentences[i].sentence, winowhy_sentences[i].wsc_id])\n",
    "        \n",
    "    acc = correct / len(winowhy_sentences)\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KB_DIR = \"kb_extract/winowhy_atomic/\"\n",
    "for model_name in ['roberta-base-qnli']:\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    robertaconfig = ExpConfig()\n",
    "    robertaconfig.set_seed()\n",
    "    robertaconfig.set_gpu_if_possible(0)\n",
    "    robertaconfig.dataset = 'winowhy'\n",
    "    robertaconfig.winowhy_dataset_path = ROOT_DIR + WINOWHY_PATH\n",
    "    robertaconfig.atomic_dataset_path = ROOT_DIR + KB_DIR + model_name + \"-atomic-category.json\"\n",
    "    robertaconfig.task_name = 'Test on WinoWhy'\n",
    "    robertaconfig.model_name = model_name\n",
    "    robertaconfig.model_path = CACHE_DIR + model_name\n",
    "    \n",
    "    print('\\n================================')\n",
    "    print('Experiment: {} using {}'.format(robertaconfig.task_name, robertaconfig.model_name))\n",
    "    \n",
    "    scores = roberta_for_winowhy(robertaconfig, roberta_cross_entropy_for_winowhy_atomic_nli)\n",
    "    \n",
    "    print('Scores: {}'.format(scores))\n",
    "    print('================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roberta+QNLI+ConceptNet(full set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInput(wl_sentence: WinoWhySentence, conceptnet: dict, tokenizer: RobertaTokenizer, max_seq_len: int, topk: int, device: str='cpu'):\n",
    "    MAX_SEQ_LEN = max_seq_len\n",
    "    \n",
    "    second = tokenizer.tokenize(wl_sentence.wsc_marked_sentence, add_prefix_space=True)\n",
    "\n",
    "    first = tokenizer.tokenize(wl_sentence.reason, add_prefix_space=True)\n",
    "    \n",
    "    input_ids = torch.tensor([]).to(device).long()\n",
    "    attention_mask = torch.tensor([]).to(device).long()\n",
    "    for i in range(topk):\n",
    "        a = tokenizer.tokenize(conceptnet['Overall'][i], add_prefix_space=True)\n",
    "        \n",
    "        input_ids_a = tokenizer.convert_tokens_to_ids([tokenizer.cls_token] + first + [tokenizer.sep_token] + a + [tokenizer.sep_token] + second + [tokenizer.sep_token])\n",
    "        input_ids_a = input_ids_a[:MAX_SEQ_LEN]\n",
    "        attention_mask_a = [1] * len(input_ids_a)\n",
    "        \n",
    "        input_ids_a += [1] * (MAX_SEQ_LEN - len(input_ids_a))\n",
    "        attention_mask_a += [0] * (MAX_SEQ_LEN - len(attention_mask_a))\n",
    "\n",
    "        input_ids_a = torch.tensor([input_ids_a]).to(device)\n",
    "        attention_mask_a = torch.tensor([attention_mask_a]).to(device)\n",
    "        \n",
    "        input_ids = torch.cat((input_ids, input_ids_a), 0)\n",
    "        attention_mask = torch.cat((attention_mask, attention_mask_a), 0)\n",
    "\n",
    "    return input_ids, attention_mask\n",
    "\n",
    "def roberta_cross_entropy_for_winowhy_conceptnet_nli(wl_sentence: WinoWhySentence, conceptnet: dict, model_1: RobertaModel, model_2: RobertaOnlyClassificationHead, tokenizer: RobertaTokenizer, max_seq_len: int, device: str='cpu') -> float:\n",
    "\n",
    "    MAX_SEQ_LEN = 128\n",
    "    input_ids, attention_mask = getInput(wl_sentence, conceptnet, tokenizer, MAX_SEQ_LEN, 10, device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model_1.eval()\n",
    "        model_2.eval()\n",
    "        outputs = model_1(input_ids, attention_mask=attention_mask)[0].mean(dim=0, keepdim=True)\n",
    "        logits = model_2(outputs)\n",
    "        loss = F.softmax(logits, dim = 1)\n",
    "    return torch.argmax(logits).to('cpu').item(), loss[0][0].item(), loss[0][1].item()\n",
    "\n",
    "def roberta_for_winowhy(config: ExpConfig, roberta_cross_entropy_for_winowhy_conceptnet_nli):\n",
    "\n",
    "    winowhy_sentences = load_winowhy_from_path(config.winowhy_dataset_path)\n",
    "    \n",
    "    \n",
    "    ff = open(config.conceptnet_dataset_path) \n",
    "    conceptnet_sentences = json.loads(ff.read())\n",
    "    ff.close()\n",
    "        \n",
    "    if config.model_path is not None and config.model_path != \"\":\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(config.model_path)\n",
    "        model_1 = RobertaModel.from_pretrained(config.model_path)\n",
    "        model_2 = RobertaOnlyClassificationHead.from_pretrained(config.model_path)\n",
    "    else:\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "        model_1 = RobertaModel.from_pretrained(config.model_path)\n",
    "        model_2 = RobertaOnlyClassificationHead.from_pretrained(config.model_path)\n",
    "        \n",
    "    model_1.eval()\n",
    "    model_1.to(config.device)\n",
    "    \n",
    "    model_2.eval()\n",
    "    model_2.to(config.device)\n",
    "    \n",
    "    correct = 0\n",
    "    \n",
    "    #result = open(SAVE_DIR + config.model_name + \".csv\", \"w\")\n",
    "    #writer = csv.writer(result)\n",
    "    #writer.writerow([\"pred\", \"label\", \"ww_sentence\", \"wsc_id\"])\n",
    "    \n",
    "    for i in range(len(winowhy_sentences)):\n",
    "        pred, entail_logits, notentail_logits = roberta_cross_entropy_for_winowhy_conceptnet_nli(winowhy_sentences[i], conceptnet_sentences[str(i)], model_1, model_2, tokenizer, config.max_seq_len, config.device)\n",
    "        if pred == 0:\n",
    "            pred_str = \"entailment\"\n",
    "        elif pred == 1:\n",
    "            pred_str = \"not entailment\"\n",
    "        if pred != winowhy_sentences[i].label:\n",
    "            correct += 1\n",
    "        #writer.writerow([pred_str, winowhy_sentences[i].label, winowhy_sentences[i].sentence, winowhy_sentences[i].wsc_id])\n",
    "        \n",
    "    acc = correct / len(winowhy_sentences)\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KB_DIR = \"kb_extract/winowhy_cn/\"\n",
    "for model_name in ['roberta-base-qnli']:\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    robertaconfig = ExpConfig()\n",
    "    robertaconfig.set_seed()\n",
    "    robertaconfig.set_gpu_if_possible(0)\n",
    "    robertaconfig.dataset = 'winowhy'\n",
    "    robertaconfig.winowhy_dataset_path = ROOT_DIR + WINOWHY_PATH\n",
    "    robertaconfig.conceptnet_dataset_path = ROOT_DIR + KB_DIR + model_name + \"-conceptnet.json\"\n",
    "    robertaconfig.task_name = 'Test on WinoWhy'\n",
    "    robertaconfig.model_name = model_name\n",
    "    robertaconfig.model_path = CACHE_DIR + model_name\n",
    "    \n",
    "    print('\\n================================')\n",
    "    print('Experiment: {} using {}'.format(robertaconfig.task_name, robertaconfig.model_name))\n",
    "    \n",
    "    scores = roberta_for_winowhy(robertaconfig, roberta_cross_entropy_for_winowhy_conceptnet_nli)\n",
    "    \n",
    "    print('Scores: {}'.format(scores))\n",
    "    print('================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RoBERTa+QNLI+ConceptNet(four categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInput(wl_sentence: WinoWhySentence, conceptnet: dict, tokenizer: RobertaTokenizer, max_seq_len: int, topk: int, device: str='cpu'):\n",
    "    MAX_SEQ_LEN = max_seq_len\n",
    "    \n",
    "    second = tokenizer.tokenize(wl_sentence.wsc_marked_sentence, add_prefix_space=True)\n",
    "\n",
    "    first = tokenizer.tokenize(wl_sentence.reason, add_prefix_space=True)\n",
    "    \n",
    "    input_ids = torch.tensor([]).to(device).long()\n",
    "    attention_mask = torch.tensor([]).to(device).long()\n",
    "    for i in range(topk):\n",
    "        a = tokenizer.tokenize(conceptnet['Physical-Entity'][i], add_prefix_space=True)\n",
    "        b = tokenizer.tokenize(conceptnet['Event-Centered'][i], add_prefix_space=True)\n",
    "        c = tokenizer.tokenize(conceptnet['Social-Interaction'][i], add_prefix_space=True)\n",
    "        d = tokenizer.tokenize(conceptnet['Taxonomic-Lexical'][i], add_prefix_space=True)\n",
    "        \n",
    "        input_ids_a = tokenizer.convert_tokens_to_ids([tokenizer.cls_token] + first + [tokenizer.sep_token] + a + [tokenizer.sep_token] + second + [tokenizer.sep_token])\n",
    "        input_ids_b = tokenizer.convert_tokens_to_ids([tokenizer.cls_token] + first + [tokenizer.sep_token] + b + [tokenizer.sep_token] + second + [tokenizer.sep_token])\n",
    "        input_ids_c = tokenizer.convert_tokens_to_ids([tokenizer.cls_token] + first + [tokenizer.sep_token] + c + [tokenizer.sep_token] + second + [tokenizer.sep_token])\n",
    "        input_ids_d = tokenizer.convert_tokens_to_ids([tokenizer.cls_token] + first + [tokenizer.sep_token] + d + [tokenizer.sep_token] + second + [tokenizer.sep_token])\n",
    "\n",
    "        input_ids_a = input_ids_a[:MAX_SEQ_LEN]\n",
    "        input_ids_b = input_ids_b[:MAX_SEQ_LEN]\n",
    "        input_ids_c = input_ids_c[:MAX_SEQ_LEN]\n",
    "        input_ids_d = input_ids_d[:MAX_SEQ_LEN]\n",
    " \n",
    "        attention_mask_a = [1] * len(input_ids_a)\n",
    "        attention_mask_b = [1] * len(input_ids_b)\n",
    "        attention_mask_c = [1] * len(input_ids_c)\n",
    "        attention_mask_d = [1] * len(input_ids_d)\n",
    "\n",
    "        input_ids_a += [1] * (MAX_SEQ_LEN - len(input_ids_a))\n",
    "        input_ids_b += [1] * (MAX_SEQ_LEN - len(input_ids_b))\n",
    "        input_ids_c += [1] * (MAX_SEQ_LEN - len(input_ids_c))\n",
    "        input_ids_d += [1] * (MAX_SEQ_LEN - len(input_ids_d))\n",
    "\n",
    "        attention_mask_a += [0] * (MAX_SEQ_LEN - len(attention_mask_a))\n",
    "        attention_mask_b += [0] * (MAX_SEQ_LEN - len(attention_mask_b))\n",
    "        attention_mask_c += [0] * (MAX_SEQ_LEN - len(attention_mask_c))\n",
    "        attention_mask_d += [0] * (MAX_SEQ_LEN - len(attention_mask_d))\n",
    "\n",
    "        input_ids_a = torch.tensor([input_ids_a]).to(device)\n",
    "        input_ids_b = torch.tensor([input_ids_b]).to(device)\n",
    "        input_ids_c = torch.tensor([input_ids_c]).to(device)\n",
    "        input_ids_d = torch.tensor([input_ids_d]).to(device)\n",
    "\n",
    "        attention_mask_a = torch.tensor([attention_mask_a]).to(device)\n",
    "        attention_mask_b = torch.tensor([attention_mask_b]).to(device)\n",
    "        attention_mask_c = torch.tensor([attention_mask_c]).to(device)\n",
    "        attention_mask_d = torch.tensor([attention_mask_d]).to(device)\n",
    "        \n",
    "        input_ids = torch.cat((input_ids, input_ids_a), 0)\n",
    "        #input_ids = torch.cat((input_ids, input_ids_b), 0)\n",
    "        #input_ids = torch.cat((input_ids, input_ids_c), 0)\n",
    "        #input_ids = torch.cat((input_ids, input_ids_d), 0)\n",
    "        \n",
    "        attention_mask = torch.cat((attention_mask, attention_mask_a), 0)\n",
    "        #attention_mask = torch.cat((attention_mask, attention_mask_b), 0)\n",
    "        #attention_mask = torch.cat((attention_mask, attention_mask_c), 0)\n",
    "        #attention_mask = torch.cat((attention_mask, attention_mask_d), 0)\n",
    "\n",
    "    return input_ids, attention_mask\n",
    "\n",
    "def roberta_cross_entropy_for_winowhy_conceptnet_nli(wl_sentence: WinoWhySentence, conceptnet: dict, model_1: RobertaModel, model_2: RobertaOnlyClassificationHead, tokenizer: RobertaTokenizer, max_seq_len: int, device: str='cpu') -> float:\n",
    "\n",
    "    MAX_SEQ_LEN = 128\n",
    "    input_ids, attention_mask = getInput(wl_sentence, conceptnet, tokenizer, MAX_SEQ_LEN, 10, device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model_1.eval()\n",
    "        model_2.eval()\n",
    "        outputs = model_1(input_ids, attention_mask=attention_mask)[0].mean(dim=0, keepdim=True)\n",
    "        logits = model_2(outputs)\n",
    "        loss = F.softmax(logits, dim = 1)\n",
    "    return torch.argmax(logits).to('cpu').item(), loss[0][0].item(), loss[0][1].item()\n",
    "\n",
    "def roberta_for_winowhy(config: ExpConfig, roberta_cross_entropy_for_winowhy_conceptnet_nli):\n",
    "\n",
    "    winowhy_sentences = load_winowhy_from_path(config.winowhy_dataset_path)\n",
    "    \n",
    "    \n",
    "    ff = open(config.conceptnet_dataset_path) \n",
    "    conceptnet_sentences = json.loads(ff.read())\n",
    "    ff.close()\n",
    "        \n",
    "    if config.model_path is not None and config.model_path != \"\":\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(config.model_path)\n",
    "        model_1 = RobertaModel.from_pretrained(config.model_path)\n",
    "        model_2 = RobertaOnlyClassificationHead.from_pretrained(config.model_path)\n",
    "    else:\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "        model_1 = RobertaModel.from_pretrained(config.model_path)\n",
    "        model_2 = RobertaOnlyClassificationHead.from_pretrained(config.model_path)\n",
    "        \n",
    "    model_1.eval()\n",
    "    model_1.to(config.device)\n",
    "    \n",
    "    model_2.eval()\n",
    "    model_2.to(config.device)\n",
    "    \n",
    "    correct = 0\n",
    "    \n",
    "    #result = open(SAVE_DIR + config.model_name + \".csv\", \"w\")\n",
    "    #writer = csv.writer(result)\n",
    "    #writer.writerow([\"pred\", \"label\", \"ww_sentence\", \"wsc_id\"])\n",
    "    \n",
    "    for i in range(len(winowhy_sentences)):\n",
    "        pred, entail_logits, notentail_logits = roberta_cross_entropy_for_winowhy_conceptnet_nli(winowhy_sentences[i], conceptnet_sentences[str(i)], model_1, model_2, tokenizer, config.max_seq_len, config.device)\n",
    "        if pred == 0:\n",
    "            pred_str = \"entailment\"\n",
    "        elif pred == 1:\n",
    "            pred_str = \"not entailment\"\n",
    "        if pred != winowhy_sentences[i].label:\n",
    "            correct += 1\n",
    "        #writer.writerow([pred_str, winowhy_sentences[i].label, winowhy_sentences[i].sentence, winowhy_sentences[i].wsc_id])\n",
    "    \n",
    "    acc = correct / len(winowhy_sentences)\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KB_DIR = \"kb_extract/winowhy_cn/\"\n",
    "for model_name in ['roberta-base-qnli', 'roberta-large-qnli']:\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    robertaconfig = ExpConfig()\n",
    "    robertaconfig.set_seed()\n",
    "    robertaconfig.set_gpu_if_possible(0)\n",
    "    robertaconfig.dataset = 'winowhy'\n",
    "    robertaconfig.winowhy_dataset_path = ROOT_DIR + WINOWHY_PATH\n",
    "    robertaconfig.conceptnet_dataset_path = ROOT_DIR + KB_DIR + model_name + \"-conceptnet-category.json\"\n",
    "    robertaconfig.task_name = 'Test on WinoWhy'\n",
    "    robertaconfig.model_name = model_name\n",
    "    robertaconfig.model_path = CACHE_DIR + model_name\n",
    "    \n",
    "    print('\\n================================')\n",
    "    print('Experiment: {} using {}'.format(robertaconfig.task_name, robertaconfig.model_name))\n",
    "    \n",
    "    scores = roberta_for_winowhy(robertaconfig, roberta_cross_entropy_for_winowhy_conceptnet_nli)\n",
    "    \n",
    "    print('Scores: {}'.format(scores))\n",
    "    print('================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wino",
   "language": "python",
   "name": "wino"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
