{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import csv\n",
    "from transformers import BertPreTrainedModel, RobertaConfig\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM, RobertaForSequenceClassification, RobertaModel\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "ROOT_DIR = '~/NLI-KB/'\n",
    "CACHE_DIR = '~/.cache/'\n",
    "CSQA_PATH = 'datasets/csqa.jsonl'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# helper function: read and dump data\n",
    "def dump_jsonl(data, output_path, append=False):\n",
    "    \"\"\"\n",
    "    Write list of objects to a JSON lines file.\n",
    "    \"\"\"\n",
    "    mode = 'a+' if append else 'w'\n",
    "    with open(output_path, mode, encoding='utf-8') as f:\n",
    "        for line in data:\n",
    "            json_record = json.dumps(line, ensure_ascii=False)\n",
    "            f.write(json_record + '\\n')\n",
    "    print('Wrote {} records to {}'.format(len(data), output_path))\n",
    "\n",
    "def load_jsonl(input_path) -> list:\n",
    "    \"\"\"\n",
    "    Read list of objects from a JSON lines file.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line.rstrip('\\n|\\r')))\n",
    "    print('Loaded {} records from {}'.format(len(data), input_path))\n",
    "    return data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from dataclasses import dataclass\n",
    "      \n",
    "class CommonsenseqaSentence(object):\n",
    "    answerKey: int = 1\n",
    "    choicesA: str = None\n",
    "    choicesB: str = None\n",
    "    choicesC: str = None\n",
    "    choicesD: str = None\n",
    "    choicesE: str = None\n",
    "    question: str = None\n",
    "    qid: str = None\n",
    "\n",
    "def load_commonsenseqa_from_path(filepath: str):\n",
    "    csqa = load_jsonl(filepath)\n",
    "    answerToIndex = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4}\n",
    "    csqs_sentences = list()\n",
    "    for c in csqa:\n",
    "        s = CommonsenseqaSentence()\n",
    "        s.qid = c['id']\n",
    "        s.question = c['question']['stem']\n",
    "        s.choicesA = c['question']['choices'][0]['text']\n",
    "        s.choicesB = c['question']['choices'][1]['text']\n",
    "        s.choicesC = c['question']['choices'][2]['text']\n",
    "        s.choicesD = c['question']['choices'][3]['text']\n",
    "        s.choicesE = c['question']['choices'][4]['text']\n",
    "        s.answerKey = answerToIndex[c['answerKey']]\n",
    "        csqs_sentences.append(s)\n",
    "    return csqs_sentences    \n",
    "\n",
    "@dataclass\n",
    "class ExpConfig(object):\n",
    "    # JSONL file path\n",
    "    dataset_path: str = \"\"\n",
    "        \n",
    "    winowhy_dataset_path: str = \"\"\n",
    "        \n",
    "    atomic_dataset_path: str = \"\"\n",
    "        \n",
    "    conceptnet_dataset_path: str = \"\"\n",
    "        \n",
    "    dataset: str = \"winowhy\"\n",
    "    # Task description\n",
    "    task_name: str = \"\"\n",
    "    # Only using single GPU\n",
    "    gpu_id: int = 0\n",
    "    # Seed for random\n",
    "    seed: int = 42\n",
    "    # 'cpu', 'cuda'\n",
    "    device: str = 'cpu' \n",
    "    # \"roberta-base\", \"roberta-largbe\"\n",
    "    model_name: str = \"\"\n",
    "    # If model_path is not None or not empty, load model from model_path instead of transformers' pretrained ones\n",
    "    model_path: str = \"\"\n",
    "    # For training the classifier layer\n",
    "    learning_rate: float = 1e-3\n",
    "    # Number of total epochs\n",
    "    num_training_epochs: int = 15\n",
    "    # Max sequence length\n",
    "    max_seq_len: int = 128\n",
    "        \n",
    "    batch_size: int = 1\n",
    "\n",
    "    def set_seed(self, new_seed = None):\n",
    "        seed = self.seed if new_seed is None else new_seed\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    def set_gpu_if_possible(self, gpu_id = None):\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = 'cuda'\n",
    "            if gpu_id is not None:\n",
    "                self.device = 'cuda:{}'.format(gpu_id)\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "            \n",
    "class RobertaClassificationHead(nn.Module):\n",
    "    \"\"\"Head for sentence-level classification tasks.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "    \n",
    "class RobertaOnlyClassificationHead(BertPreTrainedModel):\n",
    "    config_class = RobertaConfig\n",
    "    base_model_prefix = \"roberta\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.roberta = RobertaModel(config)\n",
    "        self.classifier = RobertaClassificationHead(config)\n",
    "\n",
    "    def forward(self, sequence_output):\n",
    "        logits = self.classifier(sequence_output)\n",
    "        return logits \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Use RoBERTa"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def roberta_cross_entropy_for_csqa(cs_sentence: CommonsenseqaSentence, model: RobertaForMaskedLM, tokenizer: RobertaTokenizer, max_seq_len: int, device: str='cpu') -> float:\n",
    "\n",
    "    MAX_SEQ_LEN = max_seq_len\n",
    "    question = tokenizer.tokenize(\" Q: \" + cs_sentence.question, add_prefix_space=True)\n",
    "    Atoken = tokenizer.tokenize(\" A: \", add_prefix_space=True)\n",
    "    choicesA = tokenizer.tokenize(cs_sentence.choicesA, add_prefix_space=True)\n",
    "    choicesB = tokenizer.tokenize(cs_sentence.choicesB, add_prefix_space=True)\n",
    "    choicesC = tokenizer.tokenize(cs_sentence.choicesC, add_prefix_space=True)\n",
    "    choicesD = tokenizer.tokenize(cs_sentence.choicesD, add_prefix_space=True)\n",
    "    choicesE = tokenizer.tokenize(cs_sentence.choicesE, add_prefix_space=True)\n",
    "    \n",
    "    choicesA_masks = [tokenizer.mask_token] * len(choicesA)\n",
    "    choicesB_masks = [tokenizer.mask_token] * len(choicesB)\n",
    "    choicesC_masks = [tokenizer.mask_token] * len(choicesC)\n",
    "    choicesD_masks = [tokenizer.mask_token] * len(choicesD)\n",
    "    choicesE_masks = [tokenizer.mask_token] * len(choicesE)\n",
    "\n",
    "    input_ids_A = tokenizer.convert_tokens_to_ids([tokenizer.cls_token] + question + [tokenizer.sep_token] + Atoken + choicesA_masks + [tokenizer.sep_token])\n",
    "    input_ids_B = tokenizer.convert_tokens_to_ids([tokenizer.cls_token] + question + [tokenizer.sep_token] + Atoken + choicesB_masks + [tokenizer.sep_token])\n",
    "    input_ids_C = tokenizer.convert_tokens_to_ids([tokenizer.cls_token] + question + [tokenizer.sep_token] + Atoken + choicesC_masks + [tokenizer.sep_token])\n",
    "    input_ids_D = tokenizer.convert_tokens_to_ids([tokenizer.cls_token] + question + [tokenizer.sep_token] + Atoken + choicesD_masks + [tokenizer.sep_token])\n",
    "    input_ids_E = tokenizer.convert_tokens_to_ids([tokenizer.cls_token] + question + [tokenizer.sep_token] + Atoken + choicesE_masks + [tokenizer.sep_token])\n",
    "\n",
    "    masked_lm_labels_A = [-100] + [-100]*len(question) + [-100] + [-100]*len(Atoken) + tokenizer.convert_tokens_to_ids(choicesA) + [-100]\n",
    "    masked_lm_labels_B = [-100] + [-100]*len(question) + [-100] + [-100]*len(Atoken) + tokenizer.convert_tokens_to_ids(choicesA) + [-100]\n",
    "    masked_lm_labels_C = [-100] + [-100]*len(question) + [-100] + [-100]*len(Atoken) + tokenizer.convert_tokens_to_ids(choicesA) + [-100]\n",
    "    masked_lm_labels_D = [-100] + [-100]*len(question) + [-100] + [-100]*len(Atoken) + tokenizer.convert_tokens_to_ids(choicesA) + [-100]\n",
    "    masked_lm_labels_E = [-100] + [-100]*len(question) + [-100] + [-100]*len(Atoken) + tokenizer.convert_tokens_to_ids(choicesA) + [-100]\n",
    "    \n",
    "    input_ids_A = input_ids_A[:MAX_SEQ_LEN]\n",
    "    input_ids_B = input_ids_B[:MAX_SEQ_LEN]\n",
    "    input_ids_C = input_ids_C[:MAX_SEQ_LEN]\n",
    "    input_ids_D = input_ids_D[:MAX_SEQ_LEN]\n",
    "    input_ids_E = input_ids_E[:MAX_SEQ_LEN]\n",
    "    \n",
    "    masked_lm_labels_A = masked_lm_labels_A[:MAX_SEQ_LEN]\n",
    "    masked_lm_labels_B = masked_lm_labels_B[:MAX_SEQ_LEN]\n",
    "    masked_lm_labels_C = masked_lm_labels_C[:MAX_SEQ_LEN]\n",
    "    masked_lm_labels_D = masked_lm_labels_D[:MAX_SEQ_LEN]\n",
    "    masked_lm_labels_E = masked_lm_labels_E[:MAX_SEQ_LEN]\n",
    "    \n",
    "    attention_mask_A = [1] * len(input_ids_A)\n",
    "    attention_mask_B = [1] * len(input_ids_B)\n",
    "    attention_mask_C = [1] * len(input_ids_C)\n",
    "    attention_mask_D = [1] * len(input_ids_D)\n",
    "    attention_mask_E = [1] * len(input_ids_E)\n",
    "    \n",
    "    input_ids_A += [1] * (MAX_SEQ_LEN - len(input_ids_A))\n",
    "    input_ids_B += [1] * (MAX_SEQ_LEN - len(input_ids_B))\n",
    "    input_ids_C += [1] * (MAX_SEQ_LEN - len(input_ids_C))\n",
    "    input_ids_D += [1] * (MAX_SEQ_LEN - len(input_ids_D))\n",
    "    input_ids_E += [1] * (MAX_SEQ_LEN - len(input_ids_E))\n",
    "    \n",
    "    masked_lm_labels_A += [-100] * (MAX_SEQ_LEN - len(masked_lm_labels_A))\n",
    "    masked_lm_labels_B += [-100] * (MAX_SEQ_LEN - len(masked_lm_labels_B))\n",
    "    masked_lm_labels_C += [-100] * (MAX_SEQ_LEN - len(masked_lm_labels_C))\n",
    "    masked_lm_labels_D += [-100] * (MAX_SEQ_LEN - len(masked_lm_labels_D))\n",
    "    masked_lm_labels_E += [-100] * (MAX_SEQ_LEN - len(masked_lm_labels_E))\n",
    "    \n",
    "    attention_mask_A += [0] * (MAX_SEQ_LEN - len(attention_mask_A))\n",
    "    attention_mask_B += [0] * (MAX_SEQ_LEN - len(attention_mask_B))\n",
    "    attention_mask_C += [0] * (MAX_SEQ_LEN - len(attention_mask_C))\n",
    "    attention_mask_D += [0] * (MAX_SEQ_LEN - len(attention_mask_D))\n",
    "    attention_mask_E += [0] * (MAX_SEQ_LEN - len(attention_mask_E))\n",
    "    \n",
    "    input_ids_A = torch.tensor([input_ids_A]).to(device)\n",
    "    input_ids_B = torch.tensor([input_ids_B]).to(device)\n",
    "    input_ids_C = torch.tensor([input_ids_C]).to(device)\n",
    "    input_ids_D = torch.tensor([input_ids_D]).to(device)\n",
    "    input_ids_E = torch.tensor([input_ids_E]).to(device)\n",
    "    \n",
    "    masked_lm_labels_A = torch.tensor([masked_lm_labels_A]).to(device)\n",
    "    masked_lm_labels_B = torch.tensor([masked_lm_labels_B]).to(device)\n",
    "    masked_lm_labels_C = torch.tensor([masked_lm_labels_C]).to(device)\n",
    "    masked_lm_labels_D = torch.tensor([masked_lm_labels_D]).to(device)\n",
    "    masked_lm_labels_E = torch.tensor([masked_lm_labels_E]).to(device)\n",
    "    \n",
    "    attention_mask_A = torch.tensor([attention_mask_A]).to(device)\n",
    "    attention_mask_B = torch.tensor([attention_mask_B]).to(device)\n",
    "    attention_mask_C = torch.tensor([attention_mask_C]).to(device)\n",
    "    attention_mask_D = torch.tensor([attention_mask_D]).to(device)\n",
    "    attention_mask_E = torch.tensor([attention_mask_E]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        outputs_A = model(input_ids_A, attention_mask=attention_mask_A, masked_lm_labels=masked_lm_labels_A)[0].to('cpu').item()\n",
    "        outputs_B = model(input_ids_B, attention_mask=attention_mask_B, masked_lm_labels=masked_lm_labels_B)[0].to('cpu').item()\n",
    "        outputs_C = model(input_ids_C, attention_mask=attention_mask_C, masked_lm_labels=masked_lm_labels_C)[0].to('cpu').item()\n",
    "        outputs_D = model(input_ids_D, attention_mask=attention_mask_D, masked_lm_labels=masked_lm_labels_D)[0].to('cpu').item()\n",
    "        outputs_E = model(input_ids_E, attention_mask=attention_mask_E, masked_lm_labels=masked_lm_labels_E)[0].to('cpu').item()\n",
    "        scores = [outputs_A, outputs_B, outputs_C, outputs_D, outputs_E]\n",
    "        \n",
    "    return scores.index(min(scores))\n",
    "\n",
    "\n",
    "def roberta_for_csqa(config: ExpConfig, roberta_cross_entropy_for_csqa):\n",
    "\n",
    "    csqa_sentences = load_commonsenseqa_from_path(config.dataset_path)\n",
    "        \n",
    "    if config.model_path is not None and config.model_path != \"\":\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(config.model_path)\n",
    "        model = RobertaForMaskedLM.from_pretrained(config.model_path)\n",
    "    else:\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "        model = RobertaForMaskedLM.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "    model.to(config.device)\n",
    "    \n",
    "    #result = open(SAVE_DIR + config.model_name + \".csv\", \"w\")\n",
    "    #writer = csv.writer(result)\n",
    "    #writer.writerow([\"pred\", \"label\", \"question\"])\n",
    "    \n",
    "    correct = 0\n",
    "    for cs in csqa_sentences:\n",
    "        pred = roberta_cross_entropy_for_csqa(cs, model, tokenizer, config.max_seq_len, config.device)\n",
    "        \n",
    "        if pred == cs.answerKey:\n",
    "            correct += 1\n",
    "        #writer.writerow([pred, cs.answerKey, cs.question])\n",
    "            \n",
    "    acc = correct / len(csqa_sentences)\n",
    "    print(len(csqa_sentences))\n",
    "    return acc"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for model_name in ['roberta-base', 'roberta-large']:\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    robertaconfig = ExpConfig()\n",
    "    robertaconfig.set_seed()\n",
    "    robertaconfig.set_gpu_if_possible(0)\n",
    "    robertaconfig.dataset = 'commonsenseqa'\n",
    "    robertaconfig.dataset_path = ROOT_DIR + CSQA_PATH\n",
    "    robertaconfig.task_name = 'Test on CommonsenseQA'\n",
    "    robertaconfig.model_name = model_name\n",
    "    robertaconfig.model_path = CACHE_DIR + model_name\n",
    "    \n",
    "    print('\\n================================')\n",
    "    print('Experiment: {} using {}'.format(robertaconfig.task_name, robertaconfig.model_name))\n",
    "    \n",
    "    scores = roberta_for_csqa(robertaconfig, roberta_cross_entropy_for_csqa)\n",
    "    \n",
    "    print('Scores: {}'.format(scores))\n",
    "    print('================================')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wino",
   "language": "python",
   "name": "wino"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}