Environment:
torch==1.8.1
transformers==2.11.0
numpy==1.17.4
scikit-learn==0.22.2.post1
scipy==1.3.3

Usage:

# roberta-base-mnli
python run_nli_roberta.py --data_dir="~/.cache/MNLI" --roberta_model_dir="~/.cache/roberta-base" --task_name="mnli" --output_dir="~/.cache/roberta-base-mnli" --max_seq_length=128 --do_train --do_eval --train_batch_size=32 --learning_rate=2e-5 --num_train_epochs=3 --warmup_proportion=0.15 --gpuid=0

# roberta-base-qnli
python run_nli_roberta.py --data_dir="~/.cache/QNLI" --roberta_model_dir="~/.cache/roberta-base" --task_name="qnli" --output_dir="~/.cache/roberta-base-qnli" --max_seq_length=128 --do_train --do_eval --train_batch_size=32 --learning_rate=2e-5 --num_train_epochs=3 --warmup_proportion=0.15 --gpuid=0

# roberta-large-mnli
python run_nli_roberta.py --data_dir="~/.cache/MNLI" --roberta_model_dir="~/.cache/roberta-large" --task_name="mnli" --output_dir="~/.cache/roberta-large-mnli" --max_seq_length=128 --do_train --do_eval --train_batch_size=32 --learning_rate=3e-5 --num_train_epochs=3 --warmup_proportion=0.15 --gpuid=0

# roberta-large-qnli
python run_nli_roberta.py --data_dir="~/.cache/QNLI" --roberta_model_dir="~/.cache/roberta-large" --task_name="qnli" --output_dir="~/.cache/roberta-large-qnli" --max_seq_length=128 --do_train --do_eval --train_batch_size=32 --learning_rate=3e-5 --num_train_epochs=3 --warmup_proportion=0.15 --gpuid=0