{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import csv\n",
    "from transformers import BertPreTrainedModel, RobertaConfig\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM, RobertaForSequenceClassification, RobertaModel\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "ROOT_DIR = '~/NLI-KB/'\n",
    "CACHE_DIR = '~/.cache/'\n",
    "ATOMIC_PATH= 'datasets/atomic.csv'\n",
    "CN_PATH = 'datasets/conceptnet.csv'\n",
    "WINOWHY_PATH = 'datasets/winowhy/winowhy.jsonl'\n",
    "MODEL_LIST = ['roberta-base-qnli', 'roberta-base-mnli', 'roberta-large-qnli', 'roberta-large-mnli']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# helper function: read and dump data\n",
    "def dump_jsonl(data, output_path, append=False):\n",
    "    \"\"\"\n",
    "    Write list of objects to a JSON lines file.\n",
    "    \"\"\"\n",
    "    mode = 'a+' if append else 'w'\n",
    "    with open(output_path, mode, encoding='utf-8') as f:\n",
    "        for line in data:\n",
    "            json_record = json.dumps(line, ensure_ascii=False)\n",
    "            f.write(json_record + '\\n')\n",
    "    print('Wrote {} records to {}'.format(len(data), output_path))\n",
    "\n",
    "def load_jsonl(input_path) -> list:\n",
    "    \"\"\"\n",
    "    Read list of objects from a JSON lines file.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line.rstrip('\\n|\\r')))\n",
    "    print('Loaded {} records from {}'.format(len(data), input_path))\n",
    "    return data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "class WinoWhySentence(object):\n",
    "    sentence: str = None\n",
    "    context: str = None\n",
    "    wsc_sentence: str = None\n",
    "    answer_reason: str = None\n",
    "    reason: str = None\n",
    "    label: int = 0\n",
    "    wsc_id: int = 0\n",
    "    fold_num: int = 1\n",
    "    wsc_marked_sentence: str = None\n",
    "    wsc_asked_sentence: str = None\n",
    "        \n",
    "def load_winowhy_from_path(filepath: str):\n",
    "    ws = load_jsonl(filepath)\n",
    "    winowhy_sentences = list()\n",
    "    for w in ws:\n",
    "        s = WinoWhySentence()\n",
    "        s.sentence = w['sentence']\n",
    "        s.context = w['context']\n",
    "        s.wsc_sentence = w['wsc_sentence']\n",
    "        s.answer_reason = w['answer_reason']\n",
    "        s.reason = w['reason']\n",
    "        s.label = w['label']\n",
    "        s.wsc_id = w['wsc_id']\n",
    "        s.fold_num = w['fold_num']\n",
    "        s.wsc_marked_sentence = w['wsc_marked_sentence']\n",
    "        s.wsc_asked_sentence = w['wsc_asked_sentence']\n",
    "        winowhy_sentences.append(s)\n",
    "    return winowhy_sentences\n",
    "\n",
    "@dataclass\n",
    "class ExpConfig(object):\n",
    "    # JSONL file path\n",
    "    dataset_path: str = \"\"\n",
    "        \n",
    "    winowhy_dataset_path: str = \"\"\n",
    "        \n",
    "    atomic_dataset_path: str = \"\"\n",
    "        \n",
    "    conceptnet_dataset_path: str = \"\"\n",
    "\n",
    "    dataset: str = \"winowhy\"\n",
    "    # Task description\n",
    "    task_name: str = \"\"\n",
    "    # Only using single GPU\n",
    "    gpu_id: int = 0\n",
    "    # Seed for random\n",
    "    seed: int = 42\n",
    "    # 'cpu', 'cuda'\n",
    "    device: str = 'cpu' \n",
    "    # \"roberta-base\", \"roberta-largbe\"\n",
    "    model_name: str = \"\"\n",
    "    # If model_path is not None or not empty, load model from model_path instead of transformers' pretrained ones\n",
    "    model_path: str = \"\"\n",
    "    # For training the classifier layer\n",
    "    learning_rate: float = 1e-3\n",
    "    # Number of total epochs\n",
    "    num_training_epochs: int = 15\n",
    "    # Max sequence length\n",
    "    max_seq_len: int = 128\n",
    "        \n",
    "    batch_size: int = 1\n",
    "\n",
    "    def set_seed(self, new_seed = None):\n",
    "        seed = self.seed if new_seed is None else new_seed\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    def set_gpu_if_possible(self, gpu_id = None):\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = 'cuda'\n",
    "            if gpu_id is not None:\n",
    "                self.device = 'cuda:{}'.format(gpu_id)\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "            \n",
    "class RobertaClassificationHead(nn.Module):\n",
    "    \"\"\"Head for sentence-level classification tasks.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "    \n",
    "class RobertaOnlyClassificationHead(BertPreTrainedModel):\n",
    "    config_class = RobertaConfig\n",
    "    base_model_prefix = \"roberta\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.roberta = RobertaModel(config)\n",
    "        self.classifier = RobertaClassificationHead(config)\n",
    "\n",
    "    def forward(self, sequence_output):\n",
    "        logits = self.classifier(sequence_output)\n",
    "        return logits \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extract sentences with TopK-similarity from ATOMIC (full set)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def model_dataloader(atomic_set, tokenizer: RobertaTokenizer, batch_size: int):\n",
    "    atomic_sens = list()\n",
    "    \n",
    "    for sen in atomic_set:\n",
    "        atomic_sen = tokenizer.encode('<s> ' + sen + ' </s>', add_special_tokens=True)\n",
    "        \n",
    "        atomic_sen = atomic_sen[:48]\n",
    "        atomic_sen = atomic_sen + [1] * (48 - len(atomic_sen))\n",
    "        atomic_sens.append(atomic_sen)\n",
    "        \n",
    "    atomic_inputs = torch.tensor(atomic_sens)\n",
    "    data = TensorDataset(atomic_inputs)\n",
    "    \n",
    "    data_sampler = SequentialSampler(data)\n",
    "    dataloader = DataLoader(data, sampler=data_sampler, batch_size=batch_size)\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def roberta_for_KB(config: ExpConfig):\n",
    "    \n",
    "    winowhy_sentences = load_winowhy_from_path(config.dataset_path)\n",
    "        \n",
    "    if config.model_path is not None and config.model_path != \"\":\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(config.model_path)\n",
    "        model = RobertaModel.from_pretrained(config.model_path)#, num_labels=3)\n",
    "    else:\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(config.model_name)\n",
    "        model = RobertaModel.from_pretrained(config.model_name)\n",
    "        \n",
    "    model.eval()\n",
    "    model.to(config.device)\n",
    "    \n",
    "    atomic = torch.tensor([])\n",
    "    atomic_set = []\n",
    "    with torch.no_grad():\n",
    "        f = open(ROOT_DIR + ATOMIC_PATH, 'r', encoding='UTF-8')\n",
    "        reader = csv.reader(f)\n",
    "        \n",
    "        for line in reader:\n",
    "            sen = line[1]\n",
    "            atomic_set.append(sen)\n",
    "        f.close()\n",
    "        atomic_dataloader = model_dataloader(atomic_set, tokenizer, batch_size=config.batch_size)\n",
    "        \n",
    "        for step, batch in enumerate(atomic_dataloader):\n",
    "            atomic_input_ids = batch[0]\n",
    "            atomic_input_ids = atomic_input_ids.to(config.device)\n",
    "            atomic_output = model(atomic_input_ids)\n",
    "            atomic_output = atomic_output[0].mean(dim=1).cpu()\n",
    "            atomic = torch.cat((atomic, atomic_output), 0)\n",
    "            if(atomic.size()[0] % 10240 == 0):\n",
    "                print(atomic.size())\n",
    "                break\n",
    "        print(atomic.size())\n",
    "    \n",
    "    ww = torch.tensor([])\n",
    "    with torch.no_grad():\n",
    "        for ws in winowhy_sentences:\n",
    "            #wsc_reason = \"<s> \" + ws.reason + \" </s> \" + ws.wsc_marked_sentence + \" </s>\" \n",
    "            #wsc_reason = ws.wsc_asked_sentence + \" \" + ws.reason\n",
    "            wsc_reason = \"<s>\" + ws.wsc_asked_sentence + \"</s>\" + ws.reason + \"</s>\"\n",
    "            ww_input = torch.tensor(tokenizer.encode(wsc_reason)).unsqueeze(0).to(config.device)  \n",
    "            ww_output = model(ww_input)\n",
    "            ww_output = ww_output[0].mean(dim=1).cpu()\n",
    "            ww = torch.cat((ww, ww_output), 0)\n",
    "        print(ww.size())\n",
    "        \n",
    "    first_norm = ww / ww.norm(dim=1)[:, None]\n",
    "    second_norm = atomic / atomic.norm(dim=1)[:, None]\n",
    "    res = torch.mm(first_norm, second_norm.transpose(0,1))\n",
    "    \n",
    "    res_value, res_index = res.topk(50, dim=1, largest=True, sorted=True)\n",
    "    print(res_index.size())\n",
    "    return res_index.numpy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import json\n",
    "for model_name in MODEL_LIST:\n",
    "    final = {'Overall': []}\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    robertaconfig = ExpConfig()\n",
    "    robertaconfig.set_seed()\n",
    "    robertaconfig.batch_size = 256\n",
    "    robertaconfig.set_gpu_if_possible(1)\n",
    "    robertaconfig.dataset = 'winowhy'\n",
    "    robertaconfig.dataset_path = ROOT_DIR + WINOWHY_PATH\n",
    "    robertaconfig.task_name = 'Test on WinoWhy'\n",
    "    robertaconfig.model_name = model_name\n",
    "    robertaconfig.model_path = CACHE_DIR + model_name\n",
    "\n",
    "    print('\\n================================')\n",
    "    print('Experiment: {} using {}'.format(robertaconfig.task_name, robertaconfig.model_name))\n",
    "\n",
    "    res = roberta_for_KB(robertaconfig)\n",
    "    final['Overall'] = res.tolist()\n",
    "    KB_DIR = \"kb_extract/winowhy_atomic/\"\n",
    "    final_dumps = json.dumps(final)\n",
    "    with open(ROOT_DIR + KB_DIR + model_name + \"-atomic-index.json\", 'w', encoding='utf-8') as f:\n",
    "        f.write(final_dumps)\n",
    "    f.close()\n",
    "    \n",
    "    f = open(ROOT_DIR + ATOMIC_PATH,'r', encoding='UTF-8')\n",
    "    reader = csv.reader(f)\n",
    "    Overall = []\n",
    "    for line in reader:\n",
    "        Overall.append(line[1])\n",
    "    print(len(Overall))\n",
    "    print('------------')\n",
    "    f.close()\n",
    "    \n",
    "    ff = open(ROOT_DIR + KB_DIR + model_name + \"-atomic-index.json\") \n",
    "    final_dict = json.loads(ff.read())\n",
    "    ff.close()\n",
    "    final = {}\n",
    "\n",
    "    for i in range(len(final_dict['Overall'])):\n",
    "        aa = list()\n",
    "        final[str(i)] = {'Overall': []}\n",
    "\n",
    "        for index in range(50):\n",
    "            a = Overall[final_dict['Overall'][i][index]]\n",
    "            aa.append(a)\n",
    "        final[str(i)]['Overall'] = aa\n",
    "\n",
    "    final_dumps = json.dumps(final)\n",
    "    with open(ROOT_DIR + KB_DIR + model_name + \"-atomic.json\", 'w', encoding='utf-8') as f:\n",
    "        f.write(final_dumps)\n",
    "    f.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extract sentences with TopK-similarity from ATOMIC (five categories)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def model_dataloader(atomic_set, tokenizer: RobertaTokenizer, batch_size: int):\n",
    "    \n",
    "    atomic_sens = list()\n",
    "    for sen in atomic_set:\n",
    "        atomic_sen = tokenizer.encode('<s> ' + sen + ' </s>', add_special_tokens=True)\n",
    "        atomic_sen = atomic_sen[:48]\n",
    "        atomic_sen = atomic_sen + [1] * (48 - len(atomic_sen))\n",
    "        atomic_sens.append(atomic_sen)\n",
    "        \n",
    "    atomic_inputs = torch.tensor(atomic_sens)\n",
    "    data = TensorDataset(atomic_inputs)\n",
    "    \n",
    "    data_sampler = SequentialSampler(data)\n",
    "    dataloader = DataLoader(data, sampler=data_sampler, batch_size=batch_size)\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def roberta_for_KB(config: ExpConfig, first_category, second_category):\n",
    "    \n",
    "    winowhy_sentences = load_winowhy_from_path(config.dataset_path)\n",
    "        \n",
    "    if config.model_path is not None and config.model_path != \"\":\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(config.model_path)\n",
    "        model = RobertaModel.from_pretrained(config.model_path)#, num_labels=3)\n",
    "    else:\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(config.model_name)\n",
    "        model = RobertaModel.from_pretrained(config.model_name)\n",
    "        \n",
    "    model.eval()\n",
    "    model.to(config.device)\n",
    "    \n",
    "    atomic = torch.tensor([])\n",
    "    atomic_set = []\n",
    "    with torch.no_grad():\n",
    "        f = open(ROOT_DIR + ATOMIC_PATH,'r', encoding='UTF-8')\n",
    "        reader = csv.reader(f)\n",
    "        for line in reader:\n",
    "            sen = line[1]\n",
    "            first_cat = line[2]\n",
    "            second_cat = line[3]\n",
    "            third_cat = line[4]\n",
    "            if first_category != '' and first_cat != first_category:\n",
    "                continue\n",
    "            if second_category != '' and second_cat != second_category:\n",
    "                continue\n",
    "            atomic_set.append(sen)\n",
    "        f.close()\n",
    "        \n",
    "        atomic_dataloader = model_dataloader(atomic_set, tokenizer, batch_size=config.batch_size)\n",
    "        \n",
    "        for step, batch in enumerate(atomic_dataloader):\n",
    "            atomic_input_ids = batch[0]\n",
    "            atomic_input_ids = atomic_input_ids.to(config.device)\n",
    "            atomic_output = model(atomic_input_ids)\n",
    "            atomic_output = atomic_output[0].mean(dim=1).cpu()\n",
    "            atomic = torch.cat((atomic, atomic_output), 0)\n",
    "            if(atomic.size()[0] % 3200 == 0):\n",
    "                print(atomic.size())\n",
    "                break\n",
    "        print(atomic.size())\n",
    "    \n",
    "    ww = torch.tensor([])\n",
    "    with torch.no_grad():\n",
    "        for ws in winowhy_sentences:\n",
    "            #wsc_reason = \"<s> \" + ws.reason + \" </s> \" + ws.wsc_marked_sentence + \" </s>\" \n",
    "            wsc_reason = \"<s>\" + ws.wsc_asked_sentence + \"</s>\" + ws.reason + \"</s>\"\n",
    "            ww_input = torch.tensor(tokenizer.encode(wsc_reason)).unsqueeze(0).to(config.device)\n",
    "            ww_output = model(ww_input)\n",
    "            ww_output = ww_output[0].mean(dim=1).cpu()\n",
    "            ww = torch.cat((ww, ww_output), 0)\n",
    "        print(ww.size())\n",
    "        \n",
    "    first_norm = ww / ww.norm(dim=1)[:, None]\n",
    "    second_norm = atomic / atomic.norm(dim=1)[:, None]\n",
    "    res = torch.mm(first_norm, second_norm.transpose(0,1))\n",
    "    \n",
    "    res_value, res_index = res.topk(50, dim=1, largest=True, sorted=True)\n",
    "    print(res_index.size())\n",
    "    return res_index.numpy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import json\n",
    "for model_name in MODEL_LIST:\n",
    "    final = {'Physical-Entity': [], 'Event-Centered': [], 'MentalState': [], 'Persona': [], 'Behavior': []}\n",
    "    for cat in [('Physical-Entity', ''), ('Event-Centered', ''), ('', 'MentalState'), ('', 'Persona'), ('', 'Behavior')]:\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        robertaconfig = ExpConfig()\n",
    "        robertaconfig.set_seed()\n",
    "        robertaconfig.batch_size = 1024\n",
    "        robertaconfig.set_gpu_if_possible(0)\n",
    "        robertaconfig.dataset = 'winowhy'\n",
    "        robertaconfig.dataset_path = ROOT_DIR + WINOWHY_PATH\n",
    "        robertaconfig.task_name = 'Test on WinoWhy, predicting KB'\n",
    "        robertaconfig.model_name = model_name\n",
    "        robertaconfig.model_path = CACHE_DIR + model_name\n",
    "\n",
    "        print('\\n================================')\n",
    "        print('Experiment: {} using {}'.format(robertaconfig.task_name, robertaconfig.model_name))\n",
    "\n",
    "        res = roberta_for_KB(robertaconfig, cat[0], cat[1])\n",
    "        if cat[0] != '':\n",
    "            final[cat[0]] = res.tolist()\n",
    "        else:\n",
    "            final[cat[1]] = res.tolist()\n",
    "\n",
    "    KB_DIR = \"kb_extract/winowhy_atomic/\"\n",
    "    final_dumps = json.dumps(final)\n",
    "    with open(ROOT_DIR + KB_DIR + model_name + \"-atomic-category-index.json\", 'w', encoding='utf-8') as f:\n",
    "        f.write(final_dumps)\n",
    "    f.close()\n",
    "\n",
    "    f = open(ROOT_DIR + ATOMIC_PATH, 'r', encoding='UTF-8')\n",
    "    reader = csv.reader(f)\n",
    "    reader.__next__()\n",
    "    PE = []\n",
    "    EC = []\n",
    "    MS = []\n",
    "    Per = []\n",
    "    Be = []\n",
    "    for line in reader:\n",
    "        if(line[2] == 'Physical-Entity'):\n",
    "            PE.append(line[1])\n",
    "        if(line[2] == 'Event-Centered'):\n",
    "            EC.append(line[1])\n",
    "        if(line[3] == 'MentalState'):\n",
    "            MS.append(line[1])\n",
    "        if(line[3] == 'Persona'):\n",
    "            Per.append(line[1])\n",
    "        if(line[3] == 'Behavior'):\n",
    "            Be.append(line[1])\n",
    "    f.close()\n",
    "    ff = open(ROOT_DIR + KB_DIR + model_name + \"-atomic-category-index.json\") \n",
    "    final_dict = json.loads(ff.read())\n",
    "    ff.close()\n",
    "\n",
    "    final = {}\n",
    "\n",
    "    for i in range(len(final_dict['Physical-Entity'])):\n",
    "        aa = list()\n",
    "        bb = list()\n",
    "        cc = list()\n",
    "        dd = list()\n",
    "        ee = list()\n",
    "        final[str(i)] = {'Physical-Entity': [], 'Event-Centered': [], 'MentalState': [], 'Persona': [], 'Behavior': []}\n",
    "\n",
    "        for index in range(50):\n",
    "            a = PE[final_dict['Physical-Entity'][i][index]]\n",
    "            b = EC[final_dict['Event-Centered'][i][index]]\n",
    "            c = MS[final_dict['MentalState'][i][index]]\n",
    "            d = Per[final_dict['Persona'][i][index]]\n",
    "            e = Be[final_dict['Behavior'][i][index]]\n",
    "            aa.append(a)\n",
    "            bb.append(b)\n",
    "            cc.append(c)\n",
    "            dd.append(d)\n",
    "            ee.append(e)\n",
    "        final[str(i)]['Physical-Entity'] = aa\n",
    "        final[str(i)]['Event-Centered'] = bb\n",
    "        final[str(i)]['MentalState'] = cc\n",
    "        final[str(i)]['Persona'] = dd\n",
    "        final[str(i)]['Behavior'] = ee\n",
    "\n",
    "\n",
    "    final_dumps = json.dumps(final)\n",
    "    with open(ROOT_DIR + KB_DIR + model_name + \"-atomic-category.json\", 'w', encoding='utf-8') as f:\n",
    "        f.write(final_dumps)\n",
    "    f.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extract sentences with TopK-similarity from ConceptNet (full set)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def model_dataloader(conceptnet_set, tokenizer: RobertaTokenizer, batch_size: int):\n",
    "    conceptnet_sens = list()\n",
    "    \n",
    "    for sen in conceptnet_set:\n",
    "        conceptnet_sen = tokenizer.encode('<s> ' + sen + ' </s>', add_special_tokens=True)\n",
    "        conceptnet_sen = conceptnet_sen[:48]\n",
    "        conceptnet_sen = conceptnet_sen + [1] * (48 - len(conceptnet_sen))\n",
    "        conceptnet_sens.append(conceptnet_sen)\n",
    "        \n",
    "    conceptnet_inputs = torch.tensor(conceptnet_sens)\n",
    "    data = TensorDataset(conceptnet_inputs)\n",
    "    \n",
    "    data_sampler = SequentialSampler(data)\n",
    "    dataloader = DataLoader(data, sampler=data_sampler, batch_size=batch_size)\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "def roberta_for_KB(config: ExpConfig, first_category):\n",
    "    \n",
    "    winowhy_sentences = load_winowhy_from_path(config.dataset_path)\n",
    "        \n",
    "    if config.model_path is not None and config.model_path != \"\":\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(config.model_path)\n",
    "        model = RobertaModel.from_pretrained(config.model_path)#, num_labels=3)\n",
    "    else:\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(config.model_name)\n",
    "        model = RobertaModel.from_pretrained(config.model_name)\n",
    "        \n",
    "    model.eval()\n",
    "    model.to(config.device)\n",
    "    \n",
    "    conceptnet = torch.tensor([])\n",
    "    conceptnet_set = []\n",
    "    with torch.no_grad():\n",
    "        f = open(ROOT_DIR + CN_PATH,'r', encoding='UTF-8')\n",
    "        reader = csv.reader(f)\n",
    "        for line in reader:\n",
    "            sen = line[1]\n",
    "            conceptnet_set.append(sen)\n",
    "        f.close()\n",
    "        conceptnet_dataloader = model_dataloader(conceptnet_set, tokenizer, batch_size=config.batch_size)\n",
    "        \n",
    "        for step, batch in enumerate(conceptnet_dataloader):\n",
    "            conceptnet_input_ids = batch[0]\n",
    "            conceptnet_input_ids = conceptnet_input_ids.to(config.device)\n",
    "            conceptnet_output = model(conceptnet_input_ids)\n",
    "            conceptnet_output = conceptnet_output[0].mean(dim=1).cpu()\n",
    "            conceptnet = torch.cat((conceptnet, conceptnet_output), 0)\n",
    "            if(conceptnet.size()[0] % 10240 == 0):\n",
    "                print(conceptnet.size())\n",
    "                break\n",
    "        print(conceptnet.size())\n",
    "\n",
    "    ww = torch.tensor([])\n",
    "    with torch.no_grad():\n",
    "        for ws in winowhy_sentences:\n",
    "            #wsc_reason = \"<s> \" + ws.reason + \" </s> \" + ws.wsc_marked_sentence + \" </s>\" \n",
    "            wsc_reason = \"<s>\" + ws.wsc_asked_sentence + \"</s>\" + ws.reason + \"</s>\"\n",
    "            ww_input = torch.tensor(tokenizer.encode(wsc_reason, add_special_tokens=True)).unsqueeze(0).to(config.device)  \n",
    "            ww_output = model(ww_input)\n",
    "            ww_output = ww_output[0].mean(dim=1).cpu()\n",
    "            ww = torch.cat((ww, ww_output), 0)\n",
    "        print(ww.size())\n",
    "        \n",
    "    first_norm = ww / ww.norm(dim=1)[:, None]\n",
    "    second_norm = conceptnet / conceptnet.norm(dim=1)[:, None]\n",
    "    res = torch.mm(first_norm, second_norm.transpose(0,1))\n",
    "    \n",
    "    res_value, res_index = res.topk(50, dim=1, largest=True, sorted=True)\n",
    "    print(res_index.size())\n",
    "    return res_index.numpy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import json\n",
    "for model_name in MODEL_LIST:\n",
    "    final = {'Overall': []}\n",
    "    for cat in ['Overall']:\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        robertaconfig = ExpConfig()\n",
    "        robertaconfig.set_seed()\n",
    "        robertaconfig.batch_size = 1024\n",
    "        robertaconfig.set_gpu_if_possible(0)\n",
    "        robertaconfig.dataset = 'winowhy'\n",
    "        robertaconfig.dataset_path = ROOT_DIR + WINOWHY_PATH\n",
    "        robertaconfig.task_name = 'Test on WinoWhy'\n",
    "        robertaconfig.model_name = model_name\n",
    "        robertaconfig.model_path = CACHE_DIR + model_name\n",
    "\n",
    "        print('\\n================================')\n",
    "        print('Experiment: {} using {}'.format(robertaconfig.task_name, robertaconfig.model_name))\n",
    "\n",
    "        res = roberta_for_KB(robertaconfig, cat)\n",
    "        final[cat] = res.tolist()\n",
    "\n",
    "    KB_DIR = \"kb_extract/winowhy_cn/\"\n",
    "    final_dumps = json.dumps(final)\n",
    "    with open(ROOT_DIR + KB_DIR + model_name + \"-conceptnet-index.json\", 'w', encoding='utf-8') as f:\n",
    "        f.write(final_dumps)\n",
    "    f.close()\n",
    "    \n",
    "    f = open(ROOT_DIR + CN_PATH, 'r', encoding='UTF-8')\n",
    "    reader = csv.reader(f)\n",
    "    Overall = []\n",
    "    for line in reader:\n",
    "        Overall.append(line[1])\n",
    "    print(len(Overall))\n",
    "    print('------------')\n",
    "    f.close()\n",
    "    ff = open(ROOT_DIR + KB_DIR + model_name + \"-conceptnet-index.json\") \n",
    "    final_dict = json.loads(ff.read())\n",
    "    ff.close()\n",
    "\n",
    "    final = {}\n",
    "    for i in range(len(final_dict['Overall'])):\n",
    "        aa = list()\n",
    "        final[str(i)] = {'Overall': []}\n",
    "\n",
    "        for index in range(50):\n",
    "            a = Overall[final_dict['Overall'][i][index]]\n",
    "            aa.append(a)\n",
    "        final[str(i)]['Overall'] = aa\n",
    "\n",
    "\n",
    "    final_dumps = json.dumps(final)\n",
    "    with open(ROOT_DIR + KB_DIR + model_name + \"-conceptnet.json\", 'w', encoding='utf-8') as f:\n",
    "        f.write(final_dumps)\n",
    "    f.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extract sentences with TopK-similarity from ConceptNet (four categories)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def model_dataloader(conceptnet_set, tokenizer: RobertaTokenizer, batch_size: int):\n",
    "    conceptnet_sens = list()\n",
    "    \n",
    "    for sen in conceptnet_set:\n",
    "        conceptnet_sen = tokenizer.encode('<s> ' + sen + ' </s>', add_special_tokens=True)\n",
    "        conceptnet_sen = conceptnet_sen[:48]\n",
    "        conceptnet_sen = conceptnet_sen + [1] * (48 - len(conceptnet_sen))\n",
    "        conceptnet_sens.append(conceptnet_sen)\n",
    "        \n",
    "    conceptnet_inputs = torch.tensor(conceptnet_sens)\n",
    "    data = TensorDataset(conceptnet_inputs)\n",
    "    \n",
    "    data_sampler = SequentialSampler(data)\n",
    "    dataloader = DataLoader(data, sampler=data_sampler, batch_size=batch_size)\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "def roberta_for_KB(config: ExpConfig, first_category):\n",
    "    \n",
    "    winowhy_sentences = load_winowhy_from_path(config.dataset_path)\n",
    "        \n",
    "    if config.model_path is not None and config.model_path != \"\":\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(config.model_path)\n",
    "        model = RobertaModel.from_pretrained(config.model_path)#, num_labels=3)\n",
    "    else:\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(config.model_name)\n",
    "        model = RobertaModel.from_pretrained(config.model_name)\n",
    "        \n",
    "    model.eval()\n",
    "    model.to(config.device)\n",
    "    \n",
    "    conceptnet = torch.tensor([])\n",
    "    conceptnet_set = []\n",
    "    with torch.no_grad():\n",
    "        f = open(ROOT_DIR + CN_PATH,'r', encoding='UTF-8')\n",
    "        reader = csv.reader(f)\n",
    "        for line in reader:\n",
    "            sen = line[1]\n",
    "            first_cat = line[2]\n",
    "            if first_cat != first_category:\n",
    "                continue\n",
    "            conceptnet_set.append(sen)\n",
    "        f.close()\n",
    "        conceptnet_dataloader = model_dataloader(conceptnet_set, tokenizer, batch_size=config.batch_size)\n",
    "        \n",
    "        for step, batch in enumerate(conceptnet_dataloader):\n",
    "            conceptnet_input_ids = batch[0]\n",
    "            conceptnet_input_ids = conceptnet_input_ids.to(config.device)\n",
    "            conceptnet_output = model(conceptnet_input_ids)\n",
    "            conceptnet_output = conceptnet_output[0].mean(dim=1).cpu()\n",
    "            conceptnet = torch.cat((conceptnet, conceptnet_output), 0)\n",
    "            if(conceptnet.size()[0] % 10240 == 0):\n",
    "                print(conceptnet.size())\n",
    "                break\n",
    "        print(conceptnet.size())\n",
    "\n",
    "    ww = torch.tensor([])\n",
    "    with torch.no_grad():\n",
    "        for ws in winowhy_sentences:\n",
    "            #wsc_reason = \"<s> \" + ws.reason + \" </s> \" + ws.wsc_marked_sentence + \" </s>\" \n",
    "            wsc_reason = \"<s>\" + ws.wsc_asked_sentence + \"</s>\" + ws.reason + \"</s>\"\n",
    "            ww_input = torch.tensor(tokenizer.encode(wsc_reason, add_special_tokens=True)).unsqueeze(0).to(config.device)  \n",
    "            ww_output = model(ww_input)\n",
    "            ww_output = ww_output[0].mean(dim=1).cpu()\n",
    "            ww = torch.cat((ww, ww_output), 0)\n",
    "        print(ww.size())\n",
    "        \n",
    "    first_norm = ww / ww.norm(dim=1)[:, None]\n",
    "    second_norm = conceptnet / conceptnet.norm(dim=1)[:, None]\n",
    "    res = torch.mm(first_norm, second_norm.transpose(0,1))\n",
    "    \n",
    "    res_value, res_index = res.topk(50, dim=1, largest=True, sorted=True)\n",
    "    print(res_index.size())\n",
    "    return res_index.numpy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import json\n",
    "for model_name in MODEL_LIST:\n",
    "    final = {'Physical-Entity': [], 'Event-Centered': [], 'Social-Interaction': [], 'Taxonomic-Lexical': []}\n",
    "    for cat in ['Physical-Entity', 'Event-Centered', 'Social-Interaction', 'Taxonomic-Lexical']:\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        robertaconfig = ExpConfig()\n",
    "        robertaconfig.set_seed()\n",
    "        robertaconfig.batch_size = 1024\n",
    "        robertaconfig.set_gpu_if_possible(0)\n",
    "        robertaconfig.dataset = 'winowhy'\n",
    "        robertaconfig.dataset_path = ROOT_DIR + WINOWHY_PATH\n",
    "        robertaconfig.task_name = 'Test on WinoWhy'\n",
    "        robertaconfig.model_name = model_name\n",
    "        robertaconfig.model_path = CACHE_DIR + model_name\n",
    "\n",
    "        print('\\n================================')\n",
    "        print('Experiment: {} using {}'.format(robertaconfig.task_name, robertaconfig.model_name))\n",
    "\n",
    "        res = roberta_for_KB(robertaconfig, cat)\n",
    "        final[cat] = res.tolist()\n",
    "    \n",
    "    KB_DIR = \"kb_extract/winowhy_cn/\"\n",
    "    final_dumps = json.dumps(final)\n",
    "    with open(ROOT_DIR + KB_DIR + model_name + \"-conceptnet-category-index.json\", 'w', encoding='utf-8') as f:\n",
    "        f.write(final_dumps)\n",
    "    f.close()\n",
    "\n",
    "    f = open(ROOT_DIR + CN_PATH,'r', encoding='UTF-8')\n",
    "    reader = csv.reader(f)\n",
    "    PE = []\n",
    "    EC = []\n",
    "    SI = []\n",
    "    TL = []\n",
    "    for line in reader:\n",
    "        if(line[2] == 'Physical-Entity'):\n",
    "            PE.append(line[1])\n",
    "        elif(line[2] == 'Event-Centered'):\n",
    "            EC.append(line[1])\n",
    "        elif(line[2] == 'Taxonomic-Lexical'):\n",
    "            TL.append(line[1])\n",
    "        elif(line[2] == 'Social-Interaction'):\n",
    "            SI.append(line[1])\n",
    "    f.close()\n",
    "    ff = open(ROOT_DIR + KB_DIR + model_name + \"-conceptnet-category-index.json\") \n",
    "    final_dict = json.loads(ff.read())\n",
    "    ff.close()\n",
    "\n",
    "    final = {}\n",
    "\n",
    "    for i in range(len(final_dict['Physical-Entity'])):\n",
    "        aa = list()\n",
    "        bb = list()\n",
    "        cc = list()\n",
    "        dd = list()\n",
    "        final[str(i)] = {'Physical-Entity': [], 'Event-Centered': [], 'Social-Interaction': [], 'Taxonomic-Lexical': []}\n",
    "\n",
    "        for index in range(50):\n",
    "            a = PE[final_dict['Physical-Entity'][i][index]]\n",
    "            b = EC[final_dict['Event-Centered'][i][index]]\n",
    "            c = SI[final_dict['Social-Interaction'][i][index]]\n",
    "            d = TL[final_dict['Taxonomic-Lexical'][i][index]]\n",
    "            aa.append(a)\n",
    "            bb.append(b)\n",
    "            cc.append(c)\n",
    "            dd.append(d)\n",
    "        final[str(i)]['Physical-Entity'] = aa\n",
    "        final[str(i)]['Event-Centered'] = bb\n",
    "        final[str(i)]['Social-Interaction'] = cc\n",
    "        final[str(i)]['Taxonomic-Lexical'] = dd\n",
    "\n",
    "\n",
    "    final_dumps = json.dumps(final)\n",
    "    with open(ROOT_DIR + KB_DIR + model_name + \"-conceptnet-category.json\", 'w', encoding='utf-8') as f:\n",
    "        f.write(final_dumps)\n",
    "    f.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wino",
   "language": "python",
   "name": "wino"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}