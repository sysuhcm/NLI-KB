{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import csv\n",
    "from transformers import BertPreTrainedModel, RobertaConfig\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM, RobertaForSequenceClassification, RobertaModel\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "ROOT_DIR = '~/NLI-KB/'\n",
    "CACHE_DIR = '~/.cache/'\n",
    "ATOMIC_PATH= 'datasets/atomic.csv'\n",
    "CN_PATH = 'datasets/conceptnet.csv'\n",
    "CSQA_PATH = 'datasets/csqa.jsonl'\n",
    "MODEL_LIST = ['roberta-base-qnli', 'roberta-base-mnli', 'roberta-large-qnli', 'roberta-large-mnli']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# helper function: read and dump data\n",
    "def dump_jsonl(data, output_path, append=False):\n",
    "    \"\"\"\n",
    "    Write list of objects to a JSON lines file.\n",
    "    \"\"\"\n",
    "    mode = 'a+' if append else 'w'\n",
    "    with open(output_path, mode, encoding='utf-8') as f:\n",
    "        for line in data:\n",
    "            json_record = json.dumps(line, ensure_ascii=False)\n",
    "            f.write(json_record + '\\n')\n",
    "    print('Wrote {} records to {}'.format(len(data), output_path))\n",
    "\n",
    "def load_jsonl(input_path) -> list:\n",
    "    \"\"\"\n",
    "    Read list of objects from a JSON lines file.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line.rstrip('\\n|\\r')))\n",
    "    print('Loaded {} records from {}'.format(len(data), input_path))\n",
    "    return data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from dataclasses import dataclass\n",
    "      \n",
    "class CommonsenseqaSentence(object):\n",
    "    answerKey: int = 1\n",
    "    choicesA: str = None\n",
    "    choicesB: str = None\n",
    "    choicesC: str = None\n",
    "    choicesD: str = None\n",
    "    choicesE: str = None\n",
    "    question: str = None\n",
    "    qid: str = None\n",
    "\n",
    "def load_commonsenseqa_from_path(filepath: str):\n",
    "    csqa = load_jsonl(filepath)\n",
    "    answerToIndex = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4}\n",
    "    csqs_sentences = list()\n",
    "    for c in csqa:\n",
    "        s = CommonsenseqaSentence()\n",
    "        s.qid = c['id']\n",
    "        s.question = c['question']['stem']\n",
    "        s.choicesA = c['question']['choices'][0]['text']\n",
    "        s.choicesB = c['question']['choices'][1]['text']\n",
    "        s.choicesC = c['question']['choices'][2]['text']\n",
    "        s.choicesD = c['question']['choices'][3]['text']\n",
    "        s.choicesE = c['question']['choices'][4]['text']\n",
    "        s.answerKey = answerToIndex[c['answerKey']]\n",
    "        csqs_sentences.append(s)\n",
    "    return csqs_sentences    \n",
    "\n",
    "@dataclass\n",
    "class ExpConfig(object):\n",
    "    # JSONL file path\n",
    "    dataset_path: str = \"\"\n",
    "        \n",
    "    winowhy_dataset_path: str = \"\"\n",
    "        \n",
    "    atomic_dataset_path: str = \"\"\n",
    "        \n",
    "    conceptnet_dataset_path: str = \"\"\n",
    "        \n",
    "    dataset: str = \"winowhy\"\n",
    "    # Task description\n",
    "    task_name: str = \"\"\n",
    "    # Only using single GPU\n",
    "    gpu_id: int = 0\n",
    "    # Seed for random\n",
    "    seed: int = 42\n",
    "    # 'cpu', 'cuda'\n",
    "    device: str = 'cpu' \n",
    "    # \"roberta-base\", \"roberta-largbe\"\n",
    "    model_name: str = \"\"\n",
    "    # If model_path is not None or not empty, load model from model_path instead of transformers' pretrained ones\n",
    "    model_path: str = \"\"\n",
    "    # For training the classifier layer\n",
    "    learning_rate: float = 1e-3\n",
    "    # Number of total epochs\n",
    "    num_training_epochs: int = 15\n",
    "    # Max sequence length\n",
    "    max_seq_len: int = 128\n",
    "        \n",
    "    batch_size: int = 1\n",
    "\n",
    "    def set_seed(self, new_seed = None):\n",
    "        seed = self.seed if new_seed is None else new_seed\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    def set_gpu_if_possible(self, gpu_id = None):\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = 'cuda'\n",
    "            if gpu_id is not None:\n",
    "                self.device = 'cuda:{}'.format(gpu_id)\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "            \n",
    "class RobertaClassificationHead(nn.Module):\n",
    "    \"\"\"Head for sentence-level classification tasks.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "    \n",
    "class RobertaOnlyClassificationHead(BertPreTrainedModel):\n",
    "    config_class = RobertaConfig\n",
    "    base_model_prefix = \"roberta\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.roberta = RobertaModel(config)\n",
    "        self.classifier = RobertaClassificationHead(config)\n",
    "\n",
    "    def forward(self, sequence_output):\n",
    "        logits = self.classifier(sequence_output)\n",
    "        return logits \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extract sentences with TopK-similarity from ATOMIC (full set)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def model_dataloader(atomic_set, tokenizer: RobertaTokenizer, batch_size: int):\n",
    "    atomic_sens = list()\n",
    "    for sen in atomic_set:\n",
    "        atomic_sen = tokenizer.encode(\"<s> \" + sen + \" </s>\", add_special_tokens=True)\n",
    "        atomic_sen = atomic_sen[:36]\n",
    "        atomic_sen = atomic_sen + [1] * (36 - len(atomic_sen))\n",
    "        atomic_sens.append(atomic_sen)\n",
    "        \n",
    "    atomic_inputs = torch.tensor(atomic_sens)\n",
    "    data = TensorDataset(atomic_inputs)\n",
    "    \n",
    "    data_sampler = SequentialSampler(data)\n",
    "    dataloader = DataLoader(data, sampler=data_sampler, batch_size=batch_size)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def roberta_for_KB(config: ExpConfig):\n",
    "    \n",
    "    csqa_sentences = load_commonsenseqa_from_path(config.dataset_path)\n",
    "        \n",
    "    if config.model_path is not None and config.model_path != \"\":\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(config.model_path)\n",
    "        model = RobertaModel.from_pretrained(config.model_path)#, num_labels=3)\n",
    "    else:\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(config.model_name)\n",
    "        model = RobertaModel.from_pretrained(config.model_name)\n",
    "    model.eval()\n",
    "    model.to(config.device)\n",
    "    \n",
    "    atomic = torch.tensor([])\n",
    "    atomic_set = []\n",
    "    with torch.no_grad():\n",
    "        f = open(ROOT_DIR + ATOMIC_PATH,'r', encoding='UTF-8')\n",
    "        reader = csv.reader(f)\n",
    "        for line in reader:\n",
    "            sen = line[1]\n",
    "            atomic_set.append(sen)\n",
    "        f.close()\n",
    "        atomic_dataloader = model_dataloader(atomic_set, tokenizer, batch_size=config.batch_size)\n",
    "        \n",
    "        for step, batch in enumerate(atomic_dataloader):\n",
    "            atomic_input_ids = batch[0]\n",
    "            atomic_input_ids = atomic_input_ids.to(config.device)\n",
    "            atomic_output = model(atomic_input_ids)\n",
    "            atomic_output = atomic_output[0].mean(dim=1).cpu()\n",
    "            atomic = torch.cat((atomic, atomic_output), 0)\n",
    "            if(atomic.size()[0] % 10240 == 0):\n",
    "                print(atomic.size())\n",
    "        print(atomic.size())\n",
    "    \n",
    "    csqa_A = torch.tensor([])\n",
    "    csqa_B = torch.tensor([])\n",
    "    csqa_C = torch.tensor([])\n",
    "    csqa_D = torch.tensor([])\n",
    "    csqa_E = torch.tensor([])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for cs in csqa_sentences:\n",
    "            # cs_sentence_A = cs.question + ' ' + cs.choicesA\n",
    "            cs_sentence_A = '<s> ' + cs.question + ' </s> ' + cs.choicesA + ' </s>'\n",
    "            cs_sentence_B = '<s> ' + cs.question + ' </s> ' + cs.choicesB + ' </s>'\n",
    "            cs_sentence_C = '<s> ' + cs.question + ' </s> ' + cs.choicesC + ' </s>'\n",
    "            cs_sentence_D = '<s> ' + cs.question + ' </s> ' + cs.choicesD + ' </s>'\n",
    "            cs_sentence_E = '<s> ' + cs.question + ' </s> ' + cs.choicesE + ' </s>'\n",
    "            \n",
    "            cs_input_A = torch.tensor(tokenizer.encode(cs_sentence_A, add_special_tokens=True)).unsqueeze(0).to(config.device)  \n",
    "            cs_input_B = torch.tensor(tokenizer.encode(cs_sentence_B, add_special_tokens=True)).unsqueeze(0).to(config.device)  \n",
    "            cs_input_C = torch.tensor(tokenizer.encode(cs_sentence_C, add_special_tokens=True)).unsqueeze(0).to(config.device)  \n",
    "            cs_input_D = torch.tensor(tokenizer.encode(cs_sentence_D, add_special_tokens=True)).unsqueeze(0).to(config.device)  \n",
    "            cs_input_E = torch.tensor(tokenizer.encode(cs_sentence_E, add_special_tokens=True)).unsqueeze(0).to(config.device)  \n",
    "            \n",
    "            cs_output_A = model(cs_input_A)[0].mean(dim=1).cpu()\n",
    "            cs_output_B = model(cs_input_B)[0].mean(dim=1).cpu()\n",
    "            cs_output_C = model(cs_input_C)[0].mean(dim=1).cpu()\n",
    "            cs_output_D = model(cs_input_D)[0].mean(dim=1).cpu()\n",
    "            cs_output_E = model(cs_input_E)[0].mean(dim=1).cpu()\n",
    "            \n",
    "            csqa_A = torch.cat((csqa_A, cs_output_A), 0)\n",
    "            csqa_B = torch.cat((csqa_B, cs_output_B), 0)\n",
    "            csqa_C = torch.cat((csqa_C, cs_output_C), 0)\n",
    "            csqa_D = torch.cat((csqa_D, cs_output_D), 0)\n",
    "            csqa_E = torch.cat((csqa_E, cs_output_E), 0)\n",
    "            \n",
    "        print(csqa_A.size(), csqa_B.size(), csqa_C.size(), csqa_D.size(), csqa_E.size())\n",
    "    first_norm_A = csqa_A / csqa_A.norm(dim=1)[:, None]\n",
    "    first_norm_B = csqa_B / csqa_B.norm(dim=1)[:, None]\n",
    "    first_norm_C = csqa_C / csqa_C.norm(dim=1)[:, None]\n",
    "    first_norm_D = csqa_D / csqa_D.norm(dim=1)[:, None]\n",
    "    first_norm_E = csqa_E / csqa_E.norm(dim=1)[:, None]\n",
    "    \n",
    "    second_norm = atomic / atomic.norm(dim=1)[:, None]\n",
    "    \n",
    "    res_A = torch.mm(first_norm_A, second_norm.transpose(0,1))\n",
    "    res_B = torch.mm(first_norm_B, second_norm.transpose(0,1))\n",
    "    res_C = torch.mm(first_norm_C, second_norm.transpose(0,1))\n",
    "    res_D = torch.mm(first_norm_D, second_norm.transpose(0,1))\n",
    "    res_E = torch.mm(first_norm_E, second_norm.transpose(0,1))\n",
    "    \n",
    "    res_value_A, res_index_A = res_A.topk(50, dim=1, largest=True, sorted=True)\n",
    "    res_value_B, res_index_B = res_B.topk(50, dim=1, largest=True, sorted=True)\n",
    "    res_value_C, res_index_C = res_C.topk(50, dim=1, largest=True, sorted=True)\n",
    "    res_value_D, res_index_D = res_D.topk(50, dim=1, largest=True, sorted=True)\n",
    "    res_value_E, res_index_E = res_E.topk(50, dim=1, largest=True, sorted=True)\n",
    "    \n",
    "    print(res_index_A.size(), res_index_B.size(), res_index_C.size(), res_index_D.size(), res_index_E.size())\n",
    "    return res_index_A.numpy(), res_index_B.numpy(), res_index_C.numpy(), res_index_D.numpy(), res_index_E.numpy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for model_name in MODEL_LIST:\n",
    "    final = {}\n",
    "    for cat in ['Overall']:\n",
    "        final[cat] = {'A': [], 'B': [], 'C': [], 'D': [], 'E': []}\n",
    "    for cat in ['Overall']:\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        robertaconfig = ExpConfig()\n",
    "        robertaconfig.set_seed()\n",
    "        robertaconfig.batch_size = 1024\n",
    "        robertaconfig.set_gpu_if_possible(0)\n",
    "        robertaconfig.dataset = 'commonsenseqa'\n",
    "        robertaconfig.dataset_path = ROOT_DIR + CSQA_PATH\n",
    "        robertaconfig.task_name = 'Test on CommonsenseQA'\n",
    "        robertaconfig.model_name = model_name\n",
    "        robertaconfig.model_path = CACHE_DIR + model_name\n",
    "\n",
    "        print('\\n================================')\n",
    "        print('Experiment: {} using {}'.format(robertaconfig.task_name, robertaconfig.model_name))\n",
    "\n",
    "        res_A, res_B, res_C, res_D, res_E = roberta_for_KB(robertaconfig)\n",
    "        final[cat]['A'] = res_A.tolist()\n",
    "        final[cat]['B'] = res_B.tolist()\n",
    "        final[cat]['C'] = res_C.tolist()\n",
    "        final[cat]['D'] = res_D.tolist()\n",
    "        final[cat]['E'] = res_E.tolist()\n",
    "\n",
    "    KB_DIR = \"kb_extract/csqa_atomic/\"\n",
    "    final_dumps = json.dumps(final)\n",
    "    with open(ROOT_DIR + KB_DIR + model_name + \"-atomic-index.json\", 'w', encoding='utf-8') as f:\n",
    "        f.write(final_dumps)\n",
    "    f.close()\n",
    "\n",
    "    f = open(ROOT_DIR + ATOMIC_PATH,'r', encoding='UTF-8')\n",
    "    reader = csv.reader(f)\n",
    "    Overall = []\n",
    "    for line in reader:\n",
    "        Overall.append(line[1])\n",
    "    print(len(Overall))\n",
    "    print('------------')\n",
    "    f.close()\n",
    "    ff = open(ROOT_DIR + KB_DIR + model_name + \"-atomic-index.json\") \n",
    "    final_dict = json.loads(ff.read())\n",
    "    ff.close()\n",
    "\n",
    "    final = {}\n",
    "\n",
    "    print(len(final_dict['Overall']['A']))\n",
    "    for i in range(len(final_dict['Overall']['A'])):\n",
    "        final[str(i)] = {}\n",
    "        for cat in ['Overall']:\n",
    "            final[str(i)][cat] = {'A': [], 'B': [], 'C': [], 'D': [], 'E': []}\n",
    "        for answerIndex in ['A', 'B', 'C', 'D', 'E']:\n",
    "            aa = list()\n",
    "            for index in range(50):\n",
    "                a = Overall[final_dict['Overall'][answerIndex][i][index]]\n",
    "                aa.append(a)\n",
    "            final[str(i)]['Overall'][answerIndex] = aa\n",
    "\n",
    "\n",
    "    final_dumps = json.dumps(final)\n",
    "    with open(ROOT_DIR + KB_DIR + model_name + \"-atomic.json\", 'w', encoding='utf-8') as f:\n",
    "        f.write(final_dumps)\n",
    "    f.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extract sentences with TopK-similarity from ATOMIC (five categories)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def model_dataloader(atomic_set, tokenizer: RobertaTokenizer, batch_size: int):\n",
    "    atomic_sens = list()\n",
    "    for sen in atomic_set:\n",
    "        atomic_sen = tokenizer.encode(\"<s> \" + sen + \" </s>\", add_special_tokens=True)\n",
    "        atomic_sen = atomic_sen[:36]\n",
    "        atomic_sen = atomic_sen + [1] * (36 - len(atomic_sen))\n",
    "        atomic_sens.append(atomic_sen)\n",
    "        \n",
    "    atomic_inputs = torch.tensor(atomic_sens)\n",
    "    data = TensorDataset(atomic_inputs)\n",
    "    \n",
    "    data_sampler = SequentialSampler(data)\n",
    "    dataloader = DataLoader(data, sampler=data_sampler, batch_size=batch_size)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def roberta_for_KB(config: ExpConfig, first_category, second_category):\n",
    "    \n",
    "    csqa_sentences = load_commonsenseqa_from_path(config.dataset_path)\n",
    "        \n",
    "    if config.model_path is not None and config.model_path != \"\":\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(config.model_path)\n",
    "        model = RobertaModel.from_pretrained(config.model_path, num_labels=3)\n",
    "    else:\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "        model = RobertaModel.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "    model.to(config.device)\n",
    "    \n",
    "    atomic = torch.tensor([])\n",
    "    atomic_set = []\n",
    "    with torch.no_grad():\n",
    "        f = open(ROOT_DIR + ATOMIC_PATH,'r', encoding='UTF-8')\n",
    "        reader = csv.reader(f)\n",
    "        for line in reader:\n",
    "            sen = line[1]\n",
    "            first_cat = line[2]\n",
    "            second_cat = line[3]\n",
    "            third_cat = line[4]\n",
    "            if first_category != '' and first_cat != first_category:\n",
    "                continue\n",
    "            if second_category != '' and second_cat != second_category:\n",
    "                continue\n",
    "            atomic_set.append(sen)\n",
    "        f.close()\n",
    "        atomic_dataloader = model_dataloader(atomic_set, tokenizer, batch_size=config.batch_size)\n",
    "        \n",
    "        for step, batch in enumerate(atomic_dataloader):\n",
    "            atomic_input_ids = batch[0]\n",
    "            atomic_input_ids = atomic_input_ids.to(config.device)\n",
    "            atomic_output = model(atomic_input_ids)\n",
    "            atomic_output = atomic_output[0].mean(dim=1).cpu()\n",
    "            atomic = torch.cat((atomic, atomic_output), 0)\n",
    "            if(atomic.size()[0] % 10240 == 0):\n",
    "                print(atomic.size())\n",
    "        print(atomic.size())\n",
    "    \n",
    "    csqa_A = torch.tensor([])\n",
    "    csqa_B = torch.tensor([])\n",
    "    csqa_C = torch.tensor([])\n",
    "    csqa_D = torch.tensor([])\n",
    "    csqa_E = torch.tensor([])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for cs in csqa_sentences:\n",
    "            cs_sentence_A = '<s> ' + cs.question + ' </s> ' + cs.choicesA + ' </s>'\n",
    "            cs_sentence_B = '<s> ' + cs.question + ' </s> ' + cs.choicesB + ' </s>'\n",
    "            cs_sentence_C = '<s> ' + cs.question + ' </s> ' + cs.choicesC + ' </s>'\n",
    "            cs_sentence_D = '<s> ' + cs.question + ' </s> ' + cs.choicesD + ' </s>'\n",
    "            cs_sentence_E = '<s> ' + cs.question + ' </s> ' + cs.choicesE + ' </s>'\n",
    "            \n",
    "            cs_input_A = torch.tensor(tokenizer.encode(cs_sentence_A, add_special_tokens=True)).unsqueeze(0).to(config.device)  \n",
    "            cs_input_B = torch.tensor(tokenizer.encode(cs_sentence_B, add_special_tokens=True)).unsqueeze(0).to(config.device)  \n",
    "            cs_input_C = torch.tensor(tokenizer.encode(cs_sentence_C, add_special_tokens=True)).unsqueeze(0).to(config.device)  \n",
    "            cs_input_D = torch.tensor(tokenizer.encode(cs_sentence_D, add_special_tokens=True)).unsqueeze(0).to(config.device)  \n",
    "            cs_input_E = torch.tensor(tokenizer.encode(cs_sentence_E, add_special_tokens=True)).unsqueeze(0).to(config.device)  \n",
    "            \n",
    "            cs_output_A = model(cs_input_A)[0].mean(dim=1).cpu()\n",
    "            cs_output_B = model(cs_input_B)[0].mean(dim=1).cpu()\n",
    "            cs_output_C = model(cs_input_C)[0].mean(dim=1).cpu()\n",
    "            cs_output_D = model(cs_input_D)[0].mean(dim=1).cpu()\n",
    "            cs_output_E = model(cs_input_E)[0].mean(dim=1).cpu()\n",
    "            \n",
    "            csqa_A = torch.cat((csqa_A, cs_output_A), 0)\n",
    "            csqa_B = torch.cat((csqa_B, cs_output_B), 0)\n",
    "            csqa_C = torch.cat((csqa_C, cs_output_C), 0)\n",
    "            csqa_D = torch.cat((csqa_D, cs_output_D), 0)\n",
    "            csqa_E = torch.cat((csqa_E, cs_output_E), 0)\n",
    "            \n",
    "        print(csqa_A.size(), csqa_B.size(), csqa_C.size(), csqa_D.size(), csqa_E.size())\n",
    "    first_norm_A = csqa_A / csqa_A.norm(dim=1)[:, None]\n",
    "    first_norm_B = csqa_B / csqa_B.norm(dim=1)[:, None]\n",
    "    first_norm_C = csqa_C / csqa_C.norm(dim=1)[:, None]\n",
    "    first_norm_D = csqa_D / csqa_D.norm(dim=1)[:, None]\n",
    "    first_norm_E = csqa_E / csqa_E.norm(dim=1)[:, None]\n",
    "    \n",
    "    second_norm = atomic / atomic.norm(dim=1)[:, None]\n",
    "    \n",
    "    res_A = torch.mm(first_norm_A, second_norm.transpose(0,1))\n",
    "    res_B = torch.mm(first_norm_B, second_norm.transpose(0,1))\n",
    "    res_C = torch.mm(first_norm_C, second_norm.transpose(0,1))\n",
    "    res_D = torch.mm(first_norm_D, second_norm.transpose(0,1))\n",
    "    res_E = torch.mm(first_norm_E, second_norm.transpose(0,1))\n",
    "    \n",
    "    res_value_A, res_index_A = res_A.topk(50, dim=1, largest=True, sorted=True)\n",
    "    res_value_B, res_index_B = res_B.topk(50, dim=1, largest=True, sorted=True)\n",
    "    res_value_C, res_index_C = res_C.topk(50, dim=1, largest=True, sorted=True)\n",
    "    res_value_D, res_index_D = res_D.topk(50, dim=1, largest=True, sorted=True)\n",
    "    res_value_E, res_index_E = res_E.topk(50, dim=1, largest=True, sorted=True)\n",
    "    \n",
    "    print(res_index_A.size(), res_index_B.size(), res_index_C.size(), res_index_D.size(), res_index_E.size())\n",
    "    return res_index_A.numpy(), res_index_B.numpy(), res_index_C.numpy(), res_index_D.numpy(), res_index_E.numpy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for model_name in MODEL_LIST:\n",
    "    final = {}\n",
    "    for cat in ['Physical-Entity', 'Event-Centered', 'MentalState', 'Persona', 'Behavior']:\n",
    "        final[cat] = {'A': [], 'B': [], 'C': [], 'D': [], 'E': []}\n",
    "    for cat in [('Physical-Entity', ''), ('Event-Centered', ''), ('', 'MentalState'), ('', 'Persona'), ('', 'Behavior')]:\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        robertaconfig = ExpConfig()\n",
    "        robertaconfig.set_seed()\n",
    "        robertaconfig.batch_size = 1024\n",
    "        robertaconfig.set_gpu_if_possible(0)\n",
    "        robertaconfig.dataset = 'commonsenseqa'\n",
    "        robertaconfig.dataset_path = ROOT_DIR + CSQA_PATH\n",
    "        robertaconfig.task_name = 'Test on CommonsenseQA'\n",
    "        robertaconfig.model_name = model_name\n",
    "        robertaconfig.model_path = CACHE_DIR + model_name\n",
    "\n",
    "        print('\\n================================')\n",
    "        print('Experiment: {} using {}'.format(robertaconfig.task_name, robertaconfig.model_name))\n",
    "\n",
    "        res_A, res_B, res_C, res_D, res_E = roberta_for_KB(robertaconfig, cat[0], cat[1])\n",
    "        if cat[0] != '':\n",
    "            final[cat[0]]['A'] = res_A.tolist()\n",
    "            final[cat[0]]['B'] = res_B.tolist()\n",
    "            final[cat[0]]['C'] = res_C.tolist()\n",
    "            final[cat[0]]['D'] = res_D.tolist()\n",
    "            final[cat[0]]['E'] = res_E.tolist()\n",
    "        else:\n",
    "            final[cat[1]]['A'] = res_A.tolist()\n",
    "            final[cat[1]]['B'] = res_B.tolist()\n",
    "            final[cat[1]]['C'] = res_C.tolist()\n",
    "            final[cat[1]]['D'] = res_D.tolist()\n",
    "            final[cat[1]]['E'] = res_E.tolist()\n",
    "\n",
    "    KB_DIR = \"kb_extract/csqa_atomic/\"\n",
    "    final_dumps = json.dumps(final)\n",
    "    with open(ROOT_DIR + KB_DIR + model_name + \"-atomic-category-index.json\", 'w', encoding='utf-8') as f:\n",
    "        f.write(final_dumps)\n",
    "    f.close()\n",
    "    \n",
    "    f = open(ROOT_DIR + ATOMIC_PATH,'r', encoding='UTF-8')\n",
    "    reader = csv.reader(f)\n",
    "    PE = []\n",
    "    EC = []\n",
    "    MS = []\n",
    "    Per = []\n",
    "    Be = []\n",
    "    for line in reader:\n",
    "        if(line[2] == 'Physical-Entity'):\n",
    "            PE.append(line[1])\n",
    "        if(line[2] == 'Event-Centered'):\n",
    "            EC.append(line[1])\n",
    "        if(line[3] == 'MentalState'):\n",
    "            MS.append(line[1])\n",
    "        if(line[3] == 'Persona'):\n",
    "            Per.append(line[1])\n",
    "        if(line[3] == 'Behavior'):\n",
    "            Be.append(line[1])\n",
    "    f.close()\n",
    "    \n",
    "    ff = open(ROOT_DIR + KB_DIR + model_name + \"-atomic-category-index.json\") \n",
    "    final_dict = json.loads(ff.read())\n",
    "    ff.close()\n",
    "\n",
    "    final = {}\n",
    "\n",
    "    for i in range(len(final_dict['Physical-Entity']['A'])):\n",
    "        final[str(i)] = {}\n",
    "        for cat in ['Physical-Entity', 'Event-Centered', 'MentalState', 'Persona', 'Behavior']:\n",
    "            final[str(i)][cat] = {'A': [], 'B': [], 'C': [], 'D': [], 'E': []}\n",
    "        for answerIndex in ['A', 'B', 'C', 'D', 'E']:\n",
    "            aa = list()\n",
    "            bb = list()\n",
    "            cc = list()\n",
    "            dd = list()\n",
    "            ee = list()\n",
    "            for index in range(50):\n",
    "                a = PE[final_dict['Physical-Entity'][answerIndex][i][index]]\n",
    "                b = EC[final_dict['Event-Centered'][answerIndex][i][index]]\n",
    "                c = MS[final_dict['MentalState'][answerIndex][i][index]]\n",
    "                d = Per[final_dict['Persona'][answerIndex][i][index]]\n",
    "                e = Be[final_dict['Behavior'][answerIndex][i][index]]\n",
    "                aa.append(a)\n",
    "                bb.append(b)\n",
    "                cc.append(c)\n",
    "                dd.append(d)\n",
    "                ee.append(e)\n",
    "            final[str(i)]['Physical-Entity'][answerIndex] = aa\n",
    "            final[str(i)]['Event-Centered'][answerIndex] = bb\n",
    "            final[str(i)]['MentalState'][answerIndex] = cc\n",
    "            final[str(i)]['Persona'][answerIndex] = dd\n",
    "            final[str(i)]['Behavior'][answerIndex] = ee\n",
    "\n",
    "    \n",
    "    final_dumps = json.dumps(final)\n",
    "    with open(ROOT_DIR + KB_DIR + model_name + \"-atomic-category.json\", 'w', encoding='utf-8') as f:\n",
    "        f.write(final_dumps)\n",
    "    f.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extract sentences with TopK-similarity from ConceptNet (full set)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def model_dataloader(conceptnet_set, tokenizer: RobertaTokenizer, batch_size: int):\n",
    "    conceptnet_sens = list()\n",
    "\n",
    "    for sen in conceptnet_set:\n",
    "        conceptnet_sen = tokenizer.encode(\"<s> \" + sen + \" </s>\", add_special_tokens=True)\n",
    "        conceptnet_sen = conceptnet_sen[:36]\n",
    "        conceptnet_sen = conceptnet_sen + [1] * (36 - len(conceptnet_sen))\n",
    "        conceptnet_sens.append(conceptnet_sen)\n",
    "        \n",
    "    conceptnet_inputs = torch.tensor(conceptnet_sens)\n",
    "    data = TensorDataset(conceptnet_inputs)\n",
    "    \n",
    "    data_sampler = SequentialSampler(data)\n",
    "    dataloader = DataLoader(data, sampler=data_sampler, batch_size=batch_size)\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def roberta_for_KB(config: ExpConfig, first_category):\n",
    "    \n",
    "    csqa_sentences = load_commonsenseqa_from_path(config.dataset_path)\n",
    "        \n",
    "    if config.model_path is not None and config.model_path != \"\":\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(config.model_path)\n",
    "        model = RobertaModel.from_pretrained(config.model_path, num_labels=3)\n",
    "    else:\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(config.model_name)\n",
    "        model = RobertaModel.from_pretrained(config.model_name)\n",
    "    model.eval()\n",
    "    model.to(config.device)\n",
    "    \n",
    "    conceptnet = torch.tensor([])\n",
    "    conceptnet_set = []\n",
    "    with torch.no_grad():\n",
    "        f = open(ROOT_DIR + CN_PATH, 'r', encoding='UTF-8')\n",
    "        reader = csv.reader(f)\n",
    "        for line in reader:\n",
    "            sen = line[1]\n",
    "            conceptnet_set.append(sen)\n",
    "        f.close()\n",
    "        conceptnet_dataloader = model_dataloader(conceptnet_set, tokenizer, batch_size=config.batch_size)\n",
    "        \n",
    "        for step, batch in enumerate(conceptnet_dataloader):\n",
    "            conceptnet_input_ids = batch[0]\n",
    "            conceptnet_input_ids = conceptnet_input_ids.to(config.device)\n",
    "            conceptnet_output = model(conceptnet_input_ids)\n",
    "            conceptnet_output = conceptnet_output[0].mean(dim=1).cpu()\n",
    "            conceptnet = torch.cat((conceptnet, conceptnet_output), 0)\n",
    "            if(conceptnet.size()[0] % 10240 == 0):\n",
    "                print(conceptnet.size())\n",
    "        print(conceptnet.size())\n",
    "        \n",
    "    csqa_A = torch.tensor([])\n",
    "    csqa_B = torch.tensor([])\n",
    "    csqa_C = torch.tensor([])\n",
    "    csqa_D = torch.tensor([])\n",
    "    csqa_E = torch.tensor([])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for cs in csqa_sentences:\n",
    "            cs_sentence_A = '<s> ' + cs.question + ' </s> ' + cs.choicesA + ' </s>'\n",
    "            cs_sentence_B = '<s> ' + cs.question + ' </s> ' + cs.choicesB + ' </s>'\n",
    "            cs_sentence_C = '<s> ' + cs.question + ' </s> ' + cs.choicesC + ' </s>'\n",
    "            cs_sentence_D = '<s> ' + cs.question + ' </s> ' + cs.choicesD + ' </s>'\n",
    "            cs_sentence_E = '<s> ' + cs.question + ' </s> ' + cs.choicesE + ' </s>'\n",
    "            \n",
    "            cs_input_A = torch.tensor(tokenizer.encode(cs_sentence_A, add_special_tokens=True)).unsqueeze(0).to(config.device)  \n",
    "            cs_input_B = torch.tensor(tokenizer.encode(cs_sentence_B, add_special_tokens=True)).unsqueeze(0).to(config.device)  \n",
    "            cs_input_C = torch.tensor(tokenizer.encode(cs_sentence_C, add_special_tokens=True)).unsqueeze(0).to(config.device)  \n",
    "            cs_input_D = torch.tensor(tokenizer.encode(cs_sentence_D, add_special_tokens=True)).unsqueeze(0).to(config.device)  \n",
    "            cs_input_E = torch.tensor(tokenizer.encode(cs_sentence_E, add_special_tokens=True)).unsqueeze(0).to(config.device)  \n",
    "            \n",
    "            cs_output_A = model(cs_input_A)[0].mean(dim=1).cpu()\n",
    "            cs_output_B = model(cs_input_B)[0].mean(dim=1).cpu()\n",
    "            cs_output_C = model(cs_input_C)[0].mean(dim=1).cpu()\n",
    "            cs_output_D = model(cs_input_D)[0].mean(dim=1).cpu()\n",
    "            cs_output_E = model(cs_input_E)[0].mean(dim=1).cpu()\n",
    "            \n",
    "            csqa_A = torch.cat((csqa_A, cs_output_A), 0)\n",
    "            csqa_B = torch.cat((csqa_B, cs_output_B), 0)\n",
    "            csqa_C = torch.cat((csqa_C, cs_output_C), 0)\n",
    "            csqa_D = torch.cat((csqa_D, cs_output_D), 0)\n",
    "            csqa_E = torch.cat((csqa_E, cs_output_E), 0)\n",
    "            \n",
    "        print(csqa_A.size(), csqa_B.size(), csqa_C.size(), csqa_D.size(), csqa_E.size())\n",
    "    first_norm_A = csqa_A / csqa_A.norm(dim=1)[:, None]\n",
    "    first_norm_B = csqa_B / csqa_B.norm(dim=1)[:, None]\n",
    "    first_norm_C = csqa_C / csqa_C.norm(dim=1)[:, None]\n",
    "    first_norm_D = csqa_D / csqa_D.norm(dim=1)[:, None]\n",
    "    first_norm_E = csqa_E / csqa_E.norm(dim=1)[:, None]\n",
    "    \n",
    "    second_norm = conceptnet / conceptnet.norm(dim=1)[:, None]\n",
    "    \n",
    "    res_A = torch.mm(first_norm_A, second_norm.transpose(0,1))\n",
    "    res_B = torch.mm(first_norm_B, second_norm.transpose(0,1))\n",
    "    res_C = torch.mm(first_norm_C, second_norm.transpose(0,1))\n",
    "    res_D = torch.mm(first_norm_D, second_norm.transpose(0,1))\n",
    "    res_E = torch.mm(first_norm_E, second_norm.transpose(0,1))\n",
    "    \n",
    "    res_value_A, res_index_A = res_A.topk(50, dim=1, largest=True, sorted=True)\n",
    "    res_value_B, res_index_B = res_B.topk(50, dim=1, largest=True, sorted=True)\n",
    "    res_value_C, res_index_C = res_C.topk(50, dim=1, largest=True, sorted=True)\n",
    "    res_value_D, res_index_D = res_D.topk(50, dim=1, largest=True, sorted=True)\n",
    "    res_value_E, res_index_E = res_E.topk(50, dim=1, largest=True, sorted=True)\n",
    "    \n",
    "    print(res_index_A.size(), res_index_B.size(), res_index_C.size(), res_index_D.size(), res_index_E.size())\n",
    "    return res_index_A.numpy(), res_index_B.numpy(), res_index_C.numpy(), res_index_D.numpy(), res_index_E.numpy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for model_name in MODEL_LIST:\n",
    "    final = {}\n",
    "    for cat in ['Overall']:\n",
    "        final[cat] = {'A': [], 'B': [], 'C': [], 'D': [], 'E': []}\n",
    "    for cat in ['Overall']:\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        robertaconfig = ExpConfig()\n",
    "        robertaconfig.set_seed()\n",
    "        robertaconfig.batch_size = 1024\n",
    "        robertaconfig.set_gpu_if_possible(0)\n",
    "        robertaconfig.dataset = 'commonsenseqa'\n",
    "        robertaconfig.dataset_path = ROOT_DIR + CSQA_PATH\n",
    "        robertaconfig.task_name = 'Test on CommonsenseQA'\n",
    "        robertaconfig.model_name = model_name\n",
    "        robertaconfig.model_path = CACHE_DIR + model_name\n",
    "\n",
    "        print('\\n================================')\n",
    "        print('Experiment: {} using {}'.format(robertaconfig.task_name, robertaconfig.model_name))\n",
    "\n",
    "        res_A, res_B, res_C, res_D, res_E = roberta_for_KB(robertaconfig, cat)\n",
    "        \n",
    "        final[cat]['A'] = res_A.tolist()\n",
    "        final[cat]['B'] = res_B.tolist()\n",
    "        final[cat]['C'] = res_C.tolist()\n",
    "        final[cat]['D'] = res_D.tolist()\n",
    "        final[cat]['E'] = res_E.tolist()\n",
    "\n",
    "    KB_DIR = \"kb_extract/csqa_cn/\"\n",
    "    final_dumps = json.dumps(final)\n",
    "    with open(ROOT_DIR + KB_DIR + model_name + \"-conceptnet-index.json\", 'w', encoding='utf-8') as f:\n",
    "        f.write(final_dumps)\n",
    "    f.close()\n",
    "    \n",
    "    f = open(ROOT_DIR + CN_PATH, 'r', encoding='UTF-8')\n",
    "    reader = csv.reader(f)\n",
    "    Overall = []\n",
    "    for line in reader:\n",
    "        Overall.append(line[1])\n",
    "    print(len(Overall))\n",
    "    print('------------')\n",
    "    f.close()\n",
    "    ff = open(ROOT_DIR + KB_DIR + model_name + \"-conceptnet-index.json\") \n",
    "    final_dict = json.loads(ff.read())\n",
    "    ff.close()\n",
    "\n",
    "    final = {}\n",
    "\n",
    "    print(len(final_dict['Overall']['A']))\n",
    "    for i in range(len(final_dict['Overall']['A'])):\n",
    "        final[str(i)] = {}\n",
    "        for cat in ['Overall']:\n",
    "            final[str(i)][cat] = {'A': [], 'B': [], 'C': [], 'D': [], 'E': []}\n",
    "        for answerIndex in ['A', 'B', 'C', 'D', 'E']:\n",
    "            aa = list()\n",
    "            for index in range(50):\n",
    "                a = Overall[final_dict['Overall'][answerIndex][i][index]]\n",
    "                aa.append(a)\n",
    "            final[str(i)]['Overall'][answerIndex] = aa\n",
    "\n",
    "\n",
    "    final_dumps = json.dumps(final)\n",
    "    with open(ROOT_DIR + KB_DIR + model_name + \"-conceptnet.json\", 'w', encoding='utf-8') as f:\n",
    "        f.write(final_dumps)\n",
    "    f.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extract sentences with TopK-similarity from ConceptNet (four categories)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def model_dataloader(conceptnet_set, tokenizer: RobertaTokenizer, batch_size: int):\n",
    "    conceptnet_sens = list()\n",
    "\n",
    "    for sen in conceptnet_set:\n",
    "        conceptnet_sen = tokenizer.encode(\"<s> \" + sen + \" </s>\", add_special_tokens=True)\n",
    "        conceptnet_sen = conceptnet_sen[:36]\n",
    "        conceptnet_sen = conceptnet_sen + [1] * (36 - len(conceptnet_sen))\n",
    "        conceptnet_sens.append(conceptnet_sen)\n",
    "        \n",
    "    conceptnet_inputs = torch.tensor(conceptnet_sens)\n",
    "    data = TensorDataset(conceptnet_inputs)\n",
    "    \n",
    "    data_sampler = SequentialSampler(data)\n",
    "    dataloader = DataLoader(data, sampler=data_sampler, batch_size=batch_size)\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def roberta_for_KB(config: ExpConfig, first_category):\n",
    "    \n",
    "    csqa_sentences = load_commonsenseqa_from_path(config.dataset_path)\n",
    "        \n",
    "    if config.model_path is not None and config.model_path != \"\":\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(config.model_path)\n",
    "        model = RobertaModel.from_pretrained(config.model_path, num_labels=3)\n",
    "    else:\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(config.model_name)\n",
    "        model = RobertaModel.from_pretrained(config.model_name)\n",
    "    model.eval()\n",
    "    model.to(config.device)\n",
    "    \n",
    "    conceptnet = torch.tensor([])\n",
    "    conceptnet_set = []\n",
    "    with torch.no_grad():\n",
    "        f = open(ROOT_DIR + CN_PATH, 'r', encoding='UTF-8')\n",
    "        reader = csv.reader(f)\n",
    "        for line in reader:\n",
    "            sen = line[1]\n",
    "            first_cat = line[2]\n",
    "            if first_cat != first_category:\n",
    "                continue\n",
    "            conceptnet_set.append(sen)\n",
    "        f.close()\n",
    "        conceptnet_dataloader = model_dataloader(conceptnet_set, tokenizer, batch_size=config.batch_size)\n",
    "        \n",
    "        for step, batch in enumerate(conceptnet_dataloader):\n",
    "            conceptnet_input_ids = batch[0]\n",
    "            conceptnet_input_ids = conceptnet_input_ids.to(config.device)\n",
    "            conceptnet_output = model(conceptnet_input_ids)\n",
    "            conceptnet_output = conceptnet_output[0].mean(dim=1).cpu()\n",
    "            conceptnet = torch.cat((conceptnet, conceptnet_output), 0)\n",
    "            if(conceptnet.size()[0] % 10240 == 0):\n",
    "                print(conceptnet.size())\n",
    "        print(conceptnet.size())\n",
    "        \n",
    "    csqa_A = torch.tensor([])\n",
    "    csqa_B = torch.tensor([])\n",
    "    csqa_C = torch.tensor([])\n",
    "    csqa_D = torch.tensor([])\n",
    "    csqa_E = torch.tensor([])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for cs in csqa_sentences:\n",
    "            cs_sentence_A = '<s> ' + cs.question + ' </s> ' + cs.choicesA + ' </s>'\n",
    "            cs_sentence_B = '<s> ' + cs.question + ' </s> ' + cs.choicesB + ' </s>'\n",
    "            cs_sentence_C = '<s> ' + cs.question + ' </s> ' + cs.choicesC + ' </s>'\n",
    "            cs_sentence_D = '<s> ' + cs.question + ' </s> ' + cs.choicesD + ' </s>'\n",
    "            cs_sentence_E = '<s> ' + cs.question + ' </s> ' + cs.choicesE + ' </s>'\n",
    "            \n",
    "            cs_input_A = torch.tensor(tokenizer.encode(cs_sentence_A, add_special_tokens=True)).unsqueeze(0).to(config.device)  \n",
    "            cs_input_B = torch.tensor(tokenizer.encode(cs_sentence_B, add_special_tokens=True)).unsqueeze(0).to(config.device)  \n",
    "            cs_input_C = torch.tensor(tokenizer.encode(cs_sentence_C, add_special_tokens=True)).unsqueeze(0).to(config.device)  \n",
    "            cs_input_D = torch.tensor(tokenizer.encode(cs_sentence_D, add_special_tokens=True)).unsqueeze(0).to(config.device)  \n",
    "            cs_input_E = torch.tensor(tokenizer.encode(cs_sentence_E, add_special_tokens=True)).unsqueeze(0).to(config.device)  \n",
    "            \n",
    "            cs_output_A = model(cs_input_A)[0].mean(dim=1).cpu()\n",
    "            cs_output_B = model(cs_input_B)[0].mean(dim=1).cpu()\n",
    "            cs_output_C = model(cs_input_C)[0].mean(dim=1).cpu()\n",
    "            cs_output_D = model(cs_input_D)[0].mean(dim=1).cpu()\n",
    "            cs_output_E = model(cs_input_E)[0].mean(dim=1).cpu()\n",
    "            \n",
    "            csqa_A = torch.cat((csqa_A, cs_output_A), 0)\n",
    "            csqa_B = torch.cat((csqa_B, cs_output_B), 0)\n",
    "            csqa_C = torch.cat((csqa_C, cs_output_C), 0)\n",
    "            csqa_D = torch.cat((csqa_D, cs_output_D), 0)\n",
    "            csqa_E = torch.cat((csqa_E, cs_output_E), 0)\n",
    "            \n",
    "        print(csqa_A.size(), csqa_B.size(), csqa_C.size(), csqa_D.size(), csqa_E.size())\n",
    "    first_norm_A = csqa_A / csqa_A.norm(dim=1)[:, None]\n",
    "    first_norm_B = csqa_B / csqa_B.norm(dim=1)[:, None]\n",
    "    first_norm_C = csqa_C / csqa_C.norm(dim=1)[:, None]\n",
    "    first_norm_D = csqa_D / csqa_D.norm(dim=1)[:, None]\n",
    "    first_norm_E = csqa_E / csqa_E.norm(dim=1)[:, None]\n",
    "    \n",
    "    second_norm = conceptnet / conceptnet.norm(dim=1)[:, None]\n",
    "    \n",
    "    res_A = torch.mm(first_norm_A, second_norm.transpose(0,1))\n",
    "    res_B = torch.mm(first_norm_B, second_norm.transpose(0,1))\n",
    "    res_C = torch.mm(first_norm_C, second_norm.transpose(0,1))\n",
    "    res_D = torch.mm(first_norm_D, second_norm.transpose(0,1))\n",
    "    res_E = torch.mm(first_norm_E, second_norm.transpose(0,1))\n",
    "    \n",
    "    res_value_A, res_index_A = res_A.topk(50, dim=1, largest=True, sorted=True)\n",
    "    res_value_B, res_index_B = res_B.topk(50, dim=1, largest=True, sorted=True)\n",
    "    res_value_C, res_index_C = res_C.topk(50, dim=1, largest=True, sorted=True)\n",
    "    res_value_D, res_index_D = res_D.topk(50, dim=1, largest=True, sorted=True)\n",
    "    res_value_E, res_index_E = res_E.topk(50, dim=1, largest=True, sorted=True)\n",
    "    \n",
    "    print(res_index_A.size(), res_index_B.size(), res_index_C.size(), res_index_D.size(), res_index_E.size())\n",
    "    return res_index_A.numpy(), res_index_B.numpy(), res_index_C.numpy(), res_index_D.numpy(), res_index_E.numpy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for model_name in MODEL_LIST:\n",
    "    final = {}\n",
    "    for cat in ['Physical-Entity', 'Event-Centered', 'Social-Interaction', 'Taxonomic-Lexical']:\n",
    "        final[cat] = {'A': [], 'B': [], 'C': [], 'D': [], 'E': []}\n",
    "    for cat in ['Physical-Entity', 'Event-Centered', 'Social-Interaction', 'Taxonomic-Lexical']:\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        robertaconfig = ExpConfig()\n",
    "        robertaconfig.set_seed()\n",
    "        robertaconfig.batch_size = 1024\n",
    "        robertaconfig.set_gpu_if_possible(0)\n",
    "        robertaconfig.dataset = 'commonsenseqa'\n",
    "        robertaconfig.dataset_path = ROOT_DIR + CSQA_PATH\n",
    "        robertaconfig.task_name = 'Test on CommonsenseQA'\n",
    "        robertaconfig.model_name = model_name\n",
    "        robertaconfig.model_path = CACHE_DIR + model_name\n",
    "\n",
    "        print('\\n================================')\n",
    "        print('Experiment: {} using {}'.format(robertaconfig.task_name, robertaconfig.model_name))\n",
    "\n",
    "        res_A, res_B, res_C, res_D, res_E = roberta_for_KB(robertaconfig, cat)\n",
    "        \n",
    "        final[cat]['A'] = res_A.tolist()\n",
    "        final[cat]['B'] = res_B.tolist()\n",
    "        final[cat]['C'] = res_C.tolist()\n",
    "        final[cat]['D'] = res_D.tolist()\n",
    "        final[cat]['E'] = res_E.tolist()\n",
    "    \n",
    "    KB_DIR = 'kb_extract/csqa_cn/'\n",
    "    final_dumps = json.dumps(final)\n",
    "    with open(ROOT_DIR + KB_DIR + model_name + \"-conceptnet-category-index.json\", 'w', encoding='utf-8') as f:\n",
    "        f.write(final_dumps)\n",
    "    f.close()\n",
    "    \n",
    "    f = open(ROOT_DIR + CN_PATH, 'r', encoding='UTF-8')\n",
    "    reader = csv.reader(f)\n",
    "    reader.__next__()\n",
    "    PE = []\n",
    "    EC = []\n",
    "    SI = []\n",
    "    TL = []\n",
    "    for line in reader:\n",
    "        if(line[2] == 'Physical-Entity'):\n",
    "            PE.append(line[1])\n",
    "        elif(line[2] == 'Event-Centered'):\n",
    "            EC.append(line[1])\n",
    "        elif(line[2] == 'Taxonomic-Lexical'):\n",
    "            TL.append(line[1])\n",
    "        elif(line[2] == 'Social-Interaction'):\n",
    "            SI.append(line[1])\n",
    "            \n",
    "    f.close()\n",
    "    ff = open(ROOT_DIR + KB_DIR + model_name + \"-conceptnet-category-index.json\") \n",
    "    final_dict = json.loads(ff.read())\n",
    "    ff.close()\n",
    "\n",
    "    final = {}\n",
    "\n",
    "    for i in range(len(final_dict['Physical-Entity']['A'])):\n",
    "        final[str(i)] = {}\n",
    "        for cat in ['Physical-Entity', 'Event-Centered', 'Social-Interaction', 'Taxonomic-Lexical']:\n",
    "            final[str(i)][cat] = {'A': [], 'B': [], 'C': [], 'D': [], 'E': []}\n",
    "        for answerIndex in ['A', 'B', 'C', 'D', 'E']:\n",
    "            aa = list()\n",
    "            bb = list()\n",
    "            cc = list()\n",
    "            dd = list()\n",
    "            for index in range(50):\n",
    "                a = PE[final_dict['Physical-Entity'][answerIndex][i][index]]\n",
    "                b = EC[final_dict['Event-Centered'][answerIndex][i][index]]\n",
    "                c = SI[final_dict['Social-Interaction'][answerIndex][i][index]]\n",
    "                d = TL[final_dict['Taxonomic-Lexical'][answerIndex][i][index]]\n",
    "                aa.append(a)\n",
    "                bb.append(b)\n",
    "                cc.append(c)\n",
    "                dd.append(d)\n",
    "            final[str(i)]['Physical-Entity'][answerIndex] = aa\n",
    "            final[str(i)]['Event-Centered'][answerIndex] = bb\n",
    "            final[str(i)]['Social-Interaction'][answerIndex] = cc\n",
    "            final[str(i)]['Taxonomic-Lexical'][answerIndex] = dd\n",
    "\n",
    "    \n",
    "    final_dumps = json.dumps(final)\n",
    "    with open(ROOT_DIR + KB_DIR + model_name + \"-conceptnet-category.json\", 'w', encoding='utf-8') as f:\n",
    "        f.write(final_dumps)\n",
    "    f.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wino",
   "language": "python",
   "name": "wino"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}